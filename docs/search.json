[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Statistics and R",
    "section": "",
    "text": "Preface - Aim and Scope"
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Basics of Statistics and R",
    "section": "What is this?",
    "text": "What is this?\nThis is not an exhaustive book featuring deep dives into the history or application of specific statistics concepts, ideas and methods. It is also not meant to be a textbook accompanying a university course - although I might use it in this way in the future.\nInstead, this is meant to be a collection of small texts and examples serving as primers on core statistical concepts, methods and workflows. The aim here is to present the basic ideas (as well as their execution in R) and explain the reasons behind these ideas, in order to give you a general understanding as to what is happening. I will also point you in the right direction if you want to dive deeper.\n\nAren’t there many resources like this already?\nYes, and no. Although a lot of books have been written about the core ideas behind statistical methods, and many web resources about the application of these methods in any given statistical program exist, these suffer from some major drawbacks:\n\nBooks are long and nobody wants to read anymore. I hope that my way of explaining stuff as concise as possible, as well as my way of writing will resonate in a way that many “dry” statistics textbooks simply can’t.\nWhile Andy Field’s (2012) is still a great book on the subject, it was published in 2012 and much of the code used is unfortunately wildly out-of-date. Don’t get me wrong, most of it will still work fine. However, better and more user-friendly ways to work in R have emerged since.\nAt the end of the day statistics is math, and math people love communicating via formulas and mathematical notations. I don’t know about you, but unfortunately my brain shuts off whenever I see more than two mathematical formulas in a text. Therefore, I will try to keep the mathematics to a minimum and instead focus on examples and comparisons to explain ideas.\nHow-To guides tend to focus mainly on the “how” of statistical methods and not on the “why” behind it. While they might make you an expert about anything a given R (Python, …) function may be able to do, you still won’t know when and how to use them - or why you might get certain weird results back. Also, nowadays LLMs can teach you the “how” part probably better than any website ever will.\nMost statistics websites are run by professional website owners. This is not meant as a dig at them - they can do whatever they want - but I personally hate seeing ads and cookie-popups when I just want to find out why my code won’t work. Therefore, you won’t find ads or cookie banners here, because I don’t care who you are. :)"
  },
  {
    "objectID": "index.html#the-core-structure",
    "href": "index.html#the-core-structure",
    "title": "Basics of Statistics and R",
    "section": "The Core Structure",
    "text": "The Core Structure\nThis also means that this isn’t necessarily designed to be read front-to-back. Instead, I would encourage you to jump to a specific subject whenever you encounter it, and work your way from there until the end of your issues.\nHowever, I have still organized the book into subsections that (in my view) link together logically connected subjects and methods, going from simple methods and concepts on towards more complex implementations. However, the sprawling nature of statistical methods and coding means that forcing everything into a linear progression would do more harm than good. Therefore, while you may encounter basic concepts in a given topic (like “Plotting Data” in the main topic “Descriptive Statistics”), you will also find expansions of these contexts in separate topics (in this case “Data Plotting in R”).\nTo emphasize this further, everything that centers on using R, and less on statistical methodology has been organized into it’s own part in the structure. While the numbered parts deal with statistics, part R focuses on using R and RStudio. I would encourage everyone who is not familiar with R (or certain packages I frequently use) to look at this part whenever you’re not sure what I’m doing or why I’m doing things in a certain way - you’ll hopefully find an explanation there.\n\nLast modified: 2023-09-20 11:14, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nField, Andy P., Jeremy Miles, and Zoë Field. 2012. Discovering Statistics Using R. London ; Thousand Oaks, Calif: Sage."
  },
  {
    "objectID": "files/1stats_1basics.html#defining-statistics",
    "href": "files/1stats_1basics.html#defining-statistics",
    "title": "1  The Basic Idea of Statistics",
    "section": "Defining statistics",
    "text": "Defining statistics\nAlthough they all broadly revolve around the same general idea and underlying concept, you will find hundreds - if not thousands - of different definitions for the term “statistics” out there. In order for us to be able to move forward with the rest of this collection of ideas (I try my best to not call it a book as well, okay?), we should agree on a minimal definition for what we’re about to do.\nThe most basic and all-encapsulating definition of statistics I can come up with goes like this:\nStatistics is the use of mathematical methods to simplify structured data in a search for patterns and dynamics.\nDepending on your prior knowledge, this definition may run counter to your perception of statistics. That is intentional, because the world is big and my aim is to keep these pages as short as possible. However, before we can move on, I should probably elaborate on parts of this definition, in order to avoid any misconceptions down the road:\n\nMathematics: As I’ve already stated in the intro, I will try my best not to throw complex mathematical formulas and notations at you. Luckily, you won’t even have to do that much math yourself, as many functions in R automate the number crunching part. Still, math underlies all and we can’t avoid it, whether we work with numbers or anything else (which will have to be treated as numbers anyway for the math to work).\nMethods: Statistics is not about interpreting the result, it is only about obtaining it. Does that mean that interpretation doesn’t matter? Hell no! Interpretation is probably the most important step of any analysis. However, statistics can only provide clues about the relations between numbers. You have to figure out what these numbers mean depending on the context - and also, whether the methods you choose to use actually make sense in your case in the first place!\nSimplification: Whatever the data we work with looks like, it will probably feature a lot of observations and traits (or respondents and values, or whatever they’re called) contained in a 2- or more-dimensional data structure. While complex structures are nice for recording data, they are very hard to represent. As such, we use statistics to simplify whatever data we have down to general trends and easily-communicated numbers. This way, we can represent entire columns or even more complex data structures with just a few values, making it even easier to communicate.\nPatterns and dynamics: Statistics can be split into several categories (more on that later), but in general we can either describe patterns in the data we collect, or we can take one step further and try to infer what dynamics cause these patterns in the first pace (caution: “infer” is not used in the statistical sense here). While description can work with one variable or more, inference needs other variables to explain the variation seen in the data.\nStructure: This is here mostly to distinguish between so-called quantitative and qualitative research paradigms. We here in the quantitative world work with large quantities of data that (generally) follow a set and unchanging structure, unlike those silly qualitative people with their interviews and text analyses. (Don’t hurt me, qualitative people - this is a joke. Also, the two realms are not as separated as they once were: Mixed-Methods approaches can combine both to achieve unique results, and increasing computational power can bring qualitative data into the quantitative realm, as well as quantitative maths to qualitative methods).\nData: Perhaps the hardest out of all the concepts in this definition, even though it seems very easy. The truth is that data can be pretty much anything you want to use - all you need is a way to express what you’re interested in as a variable, meaning that it needs to a) have a set and unchanging global definition, b) easy to interpret labels for the values this variable can have and c) some degree of variation between these labels. We will touch on this further in the “Data types” section in a few minutes.\nResearch: Wait, this wasn’t even in the definition?! Yes, and that’s why it’s important to address it here. You see, while statistics is most definitely a part of research, you really don’t have to have a rigorous research methodology in mind for data analysis. Therefore, I have opted to leave all the parts about developing ideas and hypotheses mostly out of these pages, and am instead focusing solely on the statistical methods. Don’t worry, we will still talk about statistically validating results as well as how to use them to evaluate your hypotheses, but there will not be a theory chapter in these pages.\n\n\nThe reasons behind it all\nWith all that in mind, why do we even use statistics?\nIn a world as complex and interconnected as ours, it can be very hard to figure out what caused certain events to take place. Similarly, every event is potentially influenced by a wide range of external factors, making it very hard to predict the results of implementing a new idea or method into practice. In these situations, statistics can be a tool to shine a light upon these hidden dynamics, either to figure out why something happened the way it did after the fact, or to predict what might possibly be the results of some action (or inaction), were it to be implemented. In the context of capitalism as it exists today, statistics is an important way to both measure performance when compared to the competition, as well as a tool to search for ways to improve one’s own business. In extreme cases, companies like Google and Meta probably build most of their value out of the monetization of predicted user behaviour (i.e. serving personalized advertisement) - using collected user information as the core data set for complex statistical methods.\nWith the role that numbers and optimizations play in today’s world, statistics can also be used to justify some sort of action to third parties - or to impress people (think of claims like “9 out of 10 doctors recommend our toothpaste!”). The important thing to recognize here is that statistics and truth are two different things! If you want to, you can use statistics to make up credibility for anything you want to convey (in fact, a whopping 83.4% of statistics are made up!) - and even if you don’t, you might accidentally end up in this position by miscalculating or mis-applying a statistical concept.\nThis is another reason why statistical literacy is an important skill. If it’s that easy to generate or make up data and results, we should be able to at least roughly judge whether these results are credible or not.\n\n\nMinimal Theory: Falsification\nTurns out, I lied to you previously: There is some minimal amount of theory about theory validation that we have to address before we jump into actually doing stuff. This is due to an unfortunate truth that anyone collecting data has to deal with - although us social scientists have it worse than for example physicists, as we have to deal with humans and their answering behaviour:\nWe will never know the entire, objective truth about the things we want to analyze. Our data will always be flawed or incomplete in some way\nImagine a world where we are blessed with infinite resources for our research (i.e. mostly time and money). In theory, we should be able to research everything we want and figure out exactly why people started their businesses (or whatever our research question is). Because of our infinite resources, we can simply go out and ask every startup founder about their reasons and then produce the most all-encompassing and rigorous analysis possible, right? Wrong.\nEven though we might have infinite resources, our analysis is still limited by several potential factors: Do we even know every founder - what if we miss someone? Do we really ask all the right questions to get what we need - what if we miss an aspect of founding that could have been important? And worst of all: Do we actually trust the people we ask to tell us the entire, honest truth about their founding story?\nAt least one of these points is always a potential issue. And unfortunately, due to limited resources, we can’t even be this rigorous in our data collection to begin with! This leads to the central issue: If we can’t ask everyone the correct question, how can we claim that the statistical results we produce are an accurate representation of the world?\nThere are two things we can do to address this problem: One lies in the way we construct our statistical methods, and we will touch on this later (look for the concept of “significance”). The other one lies in the way we construct our theories. The core idea here is the following: Because of the uncertainty in our data collection, we can’t actually verify any claim we make - but we can get closer to the truth by trying our best to falsify an opposing theory, thereby adding credence to our theory.\nThe classic example in this context is about swans. The theory “all swans are white” is something we can never verify, because we can never know “all swans” - there might always be some off-colour swan somewhere on this planet, hidden from our view or not yet born, but still disproving our hypothesis.\nWhat we can do, however, is build and falsify a counter-hypothesis like “there exist non-white swans”. This way, we don’t have to deal with a claim of maximal specificity (ALL swans) and can instead be satisfied as soon as we find at least one non-white swan. This one swan is enough to prove our counter-theory (usually called a Null-hypothesis) and thereby invalidate our original theory, no matter how general or specific we might have been when constructing it.\nIn the world of statistics, this usually works out like this: We might assume that something has a positive influence on something else, for example “An increase in education leads to an increased income”. However, even though we might find in our statistical analysis that every level of education increases salaries by 250€ on average, we can’t state this as a definite claim because of all the uncertainty discussed above (Did we miss people/industries/regions/cultures/… where this is not the case? Were people honest about their education and/or salaries?). Therefore, we build the counter-claim (“Education has a negative/no effect on salaries”) and try our best to disprove this. In our case, we have shown that an effect (probably*) exists, disproving the counter-hypothesis and strengthening our theory.\nThis also means that because there is no definite proof for any theory, nothing can ever be known to be “true”. Instead, science represents our current best guess as to what is going on in the world - until this guess is itself disproven and/or a better guess comes along. If you want to know more about this method of falsification and why we use it, Popper (2002) is your guy. If you can speak German, I’d highly advise searching for the original texts rather than English translations though.\n\n\n\n\n\n\nSide Note\n\n\n\n*This is where statistical significance and probability come into play, see the chapter on those for more information."
  },
  {
    "objectID": "files/1stats_1basics.html#contents-of-this-chapter",
    "href": "files/1stats_1basics.html#contents-of-this-chapter",
    "title": "1  The Basic Idea of Statistics",
    "section": "Contents of this chapter",
    "text": "Contents of this chapter\nBut how do we even get to this point of calculating statistical analyses? To start us off, this chapter will focus on laying the ground work for what is to come. We will continue by talking about different data types (i.e. “what is the data? What do we want to measure?”) and ways of getting it (i.e. “who do we ask? How do we measure our data?”), two central concepts for most of what is to follow.\n\nLast modified: 2023-09-20 11:14, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nPopper, Karl. 2002. The Logic of Scientific Discovery (Routledge Classics). Routledge."
  },
  {
    "objectID": "files/1stats_2datatypes.html#variables",
    "href": "files/1stats_2datatypes.html#variables",
    "title": "2  Data and Variables",
    "section": "Variables",
    "text": "Variables\nAs already stated on the last page, a variable is anything that features a set and unchanging global definition, easy to interpret labels for the values it can have and some degree of variation between these labels. Some examples to illustrate what I mean:\n\nA person’s height is a variable because it has a set definition (or unit of measurement) that can be easily interpreted (we can make a pretty good guess what the answer “2m” means) different people have differing heights.\nA survey respondent’s rating on a specified scale (“On a scale from 1 to 5, how highly do you rate … ?”) can also be a variable, because the context, question and scale are entirely under our control and identical for all respondents, while we can assume variation based on the fact that different people have different preferences.\nEven text can be treated as a variable if the context is the same (i.e. all answers are in response to the same question or from the same source), although some more processing of the texts might be needed in order to assert an easier interpretation of the labels. Unsurprisingly, text varies because no two responses will be exactly identical.\n\nWhile some variables might not adhere to the same concepts if you get them from respondents (for example, height could be given in “meter” or “inch”), they can still be usable if you are able to transform them to use the same underlying definition and value scale. Similarly, you could transform text by coding it into logical groups depending on the answers, thereby giving it a set value range and easy to interpret labels.\n\nVariable classification\nIn statistics, any variable can be used in one of three ways when it comes to analysis: Dependent variables, independent variables and control variables.\n\nA dependent variable is the variable we are interested in explaining through the use of our statistical methods. It is the representation of whatever our underlying theory stipulates to be influenced by something else. Most analysis methods focus on only one dependent variable instead of multiple, as the math gets incredibly complex when you introduce multiple phenomenons that need explaining - however, this doesn’t mean it’s impossible (SEM can do it, for example). You may also see this being called the outcome variable in some research designs.\nAn independent variable is giving the presumed cause (or multiple probable causes) for variations in the dependent variable. How many independent variables to use is generally up to you, but I would advise against using too many at once, as it might complicate things unnecessarily. Having only one independent variable can also be a valid study design if your hypothesis calls for it! You may also see this being called the predictor variable in some research designs.\nA control variable is another presumed cause for variations in the dependent variable, although its main focus lies on controlling the effect of the independent variable when more complexity is introduced. As such, the focus lies not on the control variable itself, but rather on changes in the relationship between dependent and independent variables as more complexity is introduced. As such, you can easily end up with way more control variables than independent variables, although you should still set a sensible limit on the number of controls, in order to avoid overcomplicating things for no reason.\n\nUsually, dependent and independent variables can be directly extracted from your hypothesis: In our previous example (“An increase in education leads to an increased income”), income is the dependent variable as the hypothesis states that it depends on education, the independent variable. Control variables are not directly referenced in most hypotheses and instead have to be inferred from the question “What else could influence my dependent variable?” In our case, other factors influencing income could be variables like a respondent’s work experience, the average regional income or the respondent’s work arrangements (full-time vs. part-time)."
  },
  {
    "objectID": "files/1stats_2datatypes.html#data-types",
    "href": "files/1stats_2datatypes.html#data-types",
    "title": "2  Data and Variables",
    "section": "Data Types",
    "text": "Data Types\nAs we have seen above, data can come in a variety of shapes and sizes, from numbers to rankings or even text. As such, it is important to keep in mind the type of a given variable, as that will influence the statistical methods available to you in your analysis.\n\nAt the most general level, a variable can be either categorical or numeric. Whereas for numeric variables, the numbers associated with a given case have an actual inherent numerical meaning (ex. a height of “1.84m”), categorical values use numbers only as representations of categories, with the actually chosen number being completely arbitrary. These general categories can be further broken down into the following sub-categories:\n\nNominal variables are variables where the only difference between categories lies in the name of the category, with no set ranking order. As such, the ranking order of nominal variables is completely arbitrary and can be changed at any time. An example of this (cf. Field (2012) p. 8) would be “species”: Possible values might be “human”, “cat”, “dog” or “seal”, but more values as well as a different order are entirely possible and logically sensible (ex. human - seal - dog - cat), as there is no inherent quality ranking one above the other.\nIdeally, nominal variables should be constructed in an exclusive way, i.e. every case should be classified as exactly one of the given categories (going back to Field’s species example, you can be only one species, no matter how hard you try to be bat-man or cat-woman). While nominal variables can have as many values as you want - our text answers example above probably has as many unique values as there are responses - it’s usually good to break these down into groups or (even more extreme) a binary selection like “Is human? Yes/No” for easier analysis.\nCategorical variables already introduce some degree of ranking into the variable. These variables are what you’re probably most familiar with when answering survey questions: Returning this type of data are questions like “On this given scale, how would you rate your agreement to the following statements?” Answers to these questions already come with a pre-determined order of answers built-in (here, probably something like “strongly disagree” - “somewhat disagree” - “somewhat agree” - “strongly agree”).\nNon-survey-related examples of categorical variables are things like tournament placements in sports competitions. These also show the flaw in categorical data: While we can use the results of a tournament to definitively state that the 2nd place team performed better than the 4th place team, we still don’t know anything about the actual distance between the two teams - we only know that one did better than the other, but not by how much. It could have been an obvious domination of the top two teams over the entire competition or a hard-fought battle in all encounters, but we have no way of telling based solely on the categorical ranking.\nOver in the numeric realm, discrete data describes variables where an underlying value with numeric meaning exists. However (and as the name implies), values are still discrete and clearly separated from each other. As such, variables of this form usually (but not always) take whole numbers as the only valid format (ex. star ratings from 0 to 5 - although each rank is clearly separated from the others, rankings like 1.5 stars are still possible in some rating systems).\nTo address this problem, continuous data can take values of any specificity and is only limited by your ability to measure whatever you’re studying. Height or age are great examples for that, because for height you could in theory go down to the subatomic level if you tried, while your age always changes, no matter how exactly you measure it. This also means that we never “truly” measure continuous data as exact as possible - instead we abstract it to some minimum level of measurement, where it is quasi-discrete again.\nSometimes you will even encounter special cases of discrete or continuous variables described as ratio variables. These are defined by also sporting a true and meaningful zero point in addition to their other properties. As an example, your age has a true zero point (when you were born), and every change since then has some meaning relative to that zero point. In contrast, variables like “temperature” have an entirely arbitrary zero point (unless you’re measuring in Kelvin), and as such are not ratio scaled. The same goes for “years” in a general context, outside of a person’s age - It may be 2023 as I write this, but the year 0 is chosen entirely arbitrary and has no inherent meaning (sorry, Christians).\n\n\nWhy does this matter?\nBecause statistics is math and math wants to deal with numbers, having numeric data is always preferred, as it leads to relatively clean and simple mathematical operations as well as straight-forward interpretation of results. This also means that the actual type of numerical data (discrete or continuous) does not matter for the statistical matter itself, and they are largely interchangeable - where it does matter is for the interpretation of results, as continuous results for discrete values like “a person coming from this social background is estimated to have 1.4 children” make no logical sense without critical discussion.\nIn contrast, categorical data’s number equivalents are entirely arbitrary and therefore, math can be complicated or impossible. To borrow more from Andy Field: If player A with the number 10 on a football team is the best at converting penalty kicks into scoring goals, but player A is currently injured, the trainer “would not get his number 4 to give number 6 a piggy-back and then take the kick”. A more sensible approach in this case would be to look at frequencies (see section 2 for more on that) and figure out which player has the second-best penalty conversion rate, since the players’ numbers are generally arbitrary.\nThe same holds for the classic categorical case of ranking scales (like the agreement scale) - there might be an inherent order, with “strongly agree” representing more agreement than “somewhat agree”, but the distance between the two cases is entirely arbitrary. In the worst case, two respondents could still disagree on a given issue, even though they both picked the same value on the scale!"
  },
  {
    "objectID": "files/1stats_2datatypes.html#transforming-categorical-data-for-statistical-analyses",
    "href": "files/1stats_2datatypes.html#transforming-categorical-data-for-statistical-analyses",
    "title": "2  Data and Variables",
    "section": "Transforming categorical data for statistical analyses",
    "text": "Transforming categorical data for statistical analyses\n\nQuasi-numeric categorical values\nWith how prevalent and easy to implement ordinal rating scales are in surveys, scientists have come up with a work-around for this type of categorical data that allows them to be treated as numeric for the purposes of statistical analysis. The reasoning is as follows: If we give respondents enough categories to choose from and frame the labels of these categories in a way so that all categories are roughly equally distant, we can treat them as if they were normal numbers. The convention in this case is to have at least 5 different categories, as less leads to a sharp drop in result quality - in the case of our agreement scale we would therefore be lacking a scale item, which is usually solved by including a “neutral” category: “strongly disagree” - “somewhat disagree” - “indifferent” - “somewhat agree” - “strongly agree”.\nKeep in mind that even this way, the numbers used to represent the labels are entirely arbitrary. We could label them 0 to 4or 1 to 5 (or if we want to retain the positive/negative dynamic, -2 to 2). Best practice is to have the values take up consecutive numbers (i.e. not 1-3-5-7-9), and to not include a 0, as this might screw with the math later on(as 1*5 and 1*4 stay different values, whereas 0*5 and 0*4 do not).\n\n\n\n\n\n\nImportant\n\n\n\nWhile this conversion constitutes standard practice in social sciences, it is widely contested in other fields, as you’re introducing the equal distance between and objective value of your categories out of thin air, thereby artificially enriching your data. While there is some research showing that this is generally not a problem (ex. Robitzsch (2020)), be careful where you mention that you do this, as you might spark a heated discussion like this one!\n\n\n\n\nDummy variables\nBut what about nominal values that can’t be ordered? If transforming ordinal data is already this contested, surely it’s going to be even worse for nominal data, right? In short, yes - which is why we will use a different approach.\nYou see, if we follow the ideal I set above, then every nominal category should be exclusive, with every case clearly assigned to one category. If this is indeed the case, we can transform our variable into several so-called dummy variables that each capture a proportion of the data in a usable form. In essence, we’re transforming the categories into several yes/no questions, to which we then assign the numeric values 1 for “yes” and 0 for “no”. This way, we transformed our data into a numeric form without introducing artificial differentiations - we’re simply measuring presence or absence of a given category.\nThis method can also be used for categorical data, if you still want to use it for analyses but don’t have the five answer categories or can’t argue that there is equal distance between the categories (education is the example of choice here - there are differences between different education levels and degrees, but these sure aren’t equally distant from each other)\nHowever, we won’t transform every case in our data in this way. Instead, we will set one of our cases as a reference, and then code the rest as dummy variables. In practice and going back to our species list, this would mean that if we set “human” as the reference category, we would create the variables “dummy_cat” with 1 for cats and 0 for everything else, as well as “dummy_dog” and “dummy_seal”, contrasting dogs and seals with everyone else respectively. While we lose data in the individual variables (after all, we only know “cat” or “no cat”), we still retain all information if we put them together. This is also why we can exclude the reference category - it is implicitly present in the new encodings (a case that is not a cat, not a dog and not a seal has to be a human after all - at least in this data).\n\n\n\n\n\n\nSide Note\n\n\n\nChoosing the right reference category is an art in and of itself. Out of consideration for the statistical methods we use later on, your reference category should comprise a large part of the data (rule of thumb: ~10% at least) and be meaningful as a reference category. Usually the most frequently present case should be used, but especially for ordinal data it can make sense to choose the highest- or lowest-ranked category to make interpreting the statistical results that much easier.\n\n\n\nLast modified: 2023-09-20 11:14, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nField, Andy P., Jeremy Miles, and Zoë Field. 2012. Discovering Statistics Using R. London ; Thousand Oaks, Calif: Sage.\n\n\nRobitzsch, Alexander. 2020. “Why Ordinal Variables Can (Almost) Always Be Treated as Continuous Variables: Clarifying Assumptions of Robust Continuous and Ordinal Factor Analysis Estimation Methods.” Frontiers in Education 5. https://doi.org/10.3389/feduc.2020.589965."
  },
  {
    "objectID": "files/1stats_3sampling.html#we-cant-ask-everyone",
    "href": "files/1stats_3sampling.html#we-cant-ask-everyone",
    "title": "3  Populations and Sampling",
    "section": "We can’t ask everyone",
    "text": "We can’t ask everyone\nThe unfortunate truth when doing social science research is that we can’t ask everyone we are interested in about their opinions on and reasons for whatever we are interested in. I’ve touched on this in a previous section, but will expand on this here.\nIn fact, the number of survey respondents may sometimes be significantly lower than the actual number of people they seem to represent. For example, German political polls like the Sonntagsfrage usually consist of 1,000-2,000 respondents, yet should somehow represent the 60ish million people voting in Germany. How can this be?\n\nThe Theoretical Population (Grundgesamtheit)\nThere are some hard limits we have to deal with that we can never avoid, even if we have all the resources we would want. These limits are usually of a methodological nature: The surveys we design can only be answered by people who understand the language and concepts used, and should only be answered by people who have relevant information about our interests, in order to avoid producing large amounts of useless data.\n\n\n\n\n\n\nSide Note\n\n\n\nWhile it might seem as an easy fix to simply translate our survey/research questions to multiple languages to expand this theoretical population, keep in mind that you can never be 100% certain that concepts translate well between languages, as words or ideas might mean different things to different population - or in the worst case, some words might have no reasonable equivalent in the target languages!\nThus, every translation (or more general broadening of the population) leads to respondents bringing different contexts to how they answer your questions. You will get more data this way, but the quality of your theoretical underpinning might suffer significant damage in the process!\n\n\nFiguring out this theoretical maximum is usually the first step in the data collecting process, and the absolute theoretical maximum of how many people you can ask. Unfortunately, it gets even worse from here on out as you further develop your survey methodology.\n\n\nLimitation: Access\nThere are multiple kinds of survey methods you can employ to reach people. We’re probably all still familiar with street or telephone interviews, where you randomly get approached by people wanting to ask you questions. Over the last years, the balance has continuously shifted to instead use online surveys.\nOnline surveys have many benefits for researchers and respondents: They allow dynamic questioning flow (randomizing question order or only showing questions based on previous answers), provide an easily-scalable way of reaching a lot of people, usually provide data in a easily-usable format once the data collection is done, and - arguably most important - allow respondents to answer the questions whenever they find the time to do so while being very cheap to implement.\nHowever, choosing a survey methodology always limits us to a given group of respondents: We can only ask the people we actually encounter in the streets, if we conduct telephone interviews - who answers unknown calls nowadays (or even has a land line telephone anymore)? And if we conduct online surveys, what about all those people that don’t have access to the internet or a computer in the first place?\nOf course, we could combine all these different modes of surveying to reach as many people as possible, but then we run into the issue of potentially asking the same person multiple times - not to mention the insane costs of doing everything.\nThis limits our theoretical population further to the people we can actually access and provide a means to contribute to our data collection process to. Based on what you want to study, this may be more or less of an issue, but often people don’t even think about this being the case!\n\n\nLimitation: Interest and Effort\nEven if we manage to reach people, there’s no guarantee that they will answer our questions or participate in what we want to do. This disinterest can be rooted in several (non-exclusive) reasons, some of which might be:\n\nSome people just generally don’t care about or want to participate in surveys. Maybe they don’t want to put in the effort, maybe they distrust you or some general institution, or maybe they just had a shit day and don’t want to deal with people right now.\nSome people might not have the time right now, but will certainly do it later. Unfortunately (and this might sound familiar to some of you), people tend to forget what they planned or promised. It might be a good idea to keep track of who answered your questions already and send periodical reminders to the rest. This way, you might even get some people from the first group to respond, just so that you stop annoying them (or you may get blocked by them instead).\nSome people might want to answer your survey, but then they open it up, complete the first tow pages only to see that they’re stuck at 2% on the progress bar and decide that they don’t have time for this right now. There are several things you can do to counteract this: a) lie on the progress bar and make the first few pages contribute more than the rest. b) keep your survey as short as humanly possible. c) provide an estimated duration in the beginning so that respondents can plan their time accordingly.\n\n\n\n\n\n\n\nSide Note\n\n\n\nImportant: If you provide an estimated duration, don’t be too optimistic! Being stuck at 50% progress at the estimated completion time might again drive potential respondents away. Approach it conservatively and consider whether there are words or phrases that might confuse respondents, slowing down their response times!\n\n\nWhile interest is not something we can directly influence as it’s usually a core personality trait of our potential respondents, the first two factors are something we can deal with. Therefore, the theoretical population (Grundgesamtheit) for any given research question can usually be defined as:\nThe group of people we are interested in and are targeting with our questionnaire design, subsetted by the people we can actually reach with our chosen data collection method\nYou may wonder why I harp on about this. Trust me, it will all make sense in a moment, once we actually talk about …"
  },
  {
    "objectID": "files/1stats_3sampling.html#sampling",
    "href": "files/1stats_3sampling.html#sampling",
    "title": "3  Populations and Sampling",
    "section": "Sampling",
    "text": "Sampling\nEven if we have a clearly delineated theoretical population and a robust research methodology, we still won’t be able to ask everyone we care about. Going back to the Sonntagsfrage example, imagine the insanity of calling every landline telephone user in Germany on a weekly (or even monthly) basis to ask them about their political preferences. The time and financial costs would be extraordinary.\nLuckily, we can employ randomness in order to reduce the amount of effort needed. To explore this, we should first establish why we would even need to ask 60 million people to get insight into the vote distribution of 10 or less parties.\n\nTriangulation\nNo two people are alike. Everyone comes from a different social and familial background, brings their own experiences with them and has their own moral and philosophical worldviews reinforcing their beliefs. All these factors make every person relevant for our survey if we want to find out what influences their responses.\nHowever, we’re usually not interested in the entire, complex reality, but instead in a simplified subset of variables that can adequately explain most of what we see. To illustrate this, regression takes potentially complex interactions and breaks them down to a linear dependency - no matter how complex an interaction, the result of a linear regression is “Every increase in A increases B by X points.”\nBecause of this methodological simplification in our analysis, we can also simplify the types of people we are interested in. As long as we ask enough people (and these people are different enough) to give us a wide distribution over every variable and variable combination, we can get away with a way smaller amount of respondents that allow us to triangulate the actual dynamics in a simplified way.\n\n\nRandomness\nUnfortunately (notice a pattern?), we will never know all relevant variables needed for this triangulation before the fact. As such, if we manually try to pick the most varied respondents, we run the risk of artificially crippling our data on many on these unknown dimensions, potentially producing wrong results! We need a process that ensures that we get a lot of respondents that all feature a wide range of answering behaviours on all known and unknown dimensions. This is where randomness comes in.\nIf we simply pick random people from our total population to answer our survey, we may not be able to guarantee that every value of every variable of interest will be represented. But because of the randomness, we can be sure that almost any known and unknown influences are randomly distributed and therefore do not exert a systematic influence on our calculations and estimations.\nAt the same time, the issue of missing values in the distribution can be solved by simply increasing the number of people we ask: If we randomly select more people, the chance of encountering many of the possible variable combinations increases significantly."
  },
  {
    "objectID": "files/1stats_3sampling.html#differences-between-sample-and-population",
    "href": "files/1stats_3sampling.html#differences-between-sample-and-population",
    "title": "3  Populations and Sampling",
    "section": "Differences between Sample and Population",
    "text": "Differences between Sample and Population\nBecause we randomly draw respondents from all possible respondents, it is entirely possible that we only encounter a skewed proportion of responses.\nLet’s say we wanted to find out how many people are left-handed. We could simply go out and ask all people we can reach whether they are right- or left-handed, but there remains a certain chance that we will (purely coincidentally) only encounter right-handed respondents. In fact, we can quantify this chance: Given the fact that left-handedness seems to have a prevalence of about 10% (see Kovel, Carrión-Castillo, and Francks 2019), if we ask ten people, we will have a 34.87% chance of only encountering right-handed people. In fact, with only one variable to look at and a somewhat reasonable distribution, we only have to ask 100 people to bring the “risk” of only encountering right-handed people down to 0.0027%!\nHowever, this is not the whole story, as even if we encounter left-handed people, we could still be way off the “true” 10% value - we might find 5% left-handedness, or 15% or even 50% (although that is even less likely). In fact, we can simulate the proportions of left-handedness for multiple samples and different sample sizes. Here’s what happens if we draw samples of 10, 100, 1,000 and 10,000 people and ask them if they’re left-handed:\n\nlibrary(ggplot2)\n# samples to draw\nsample &lt;- 250\n# sampling function\nsample_left &lt;- function(rep_n, sample_n){\n  df &lt;- data.frame(sample = seq(1, sample_n), perc = NA)\n  for(i in 1:sample){\n    left &lt;- sample(c(1, 0), rep_n, replace = T, prob = c(0.1, 0.9))\n    df$perc[i] &lt;- mean(left)\n  }\n  return(df)\n}\n\nleft_10 &lt;- sample_left(10, sample)\nleft_100 &lt;- sample_left(100, sample)\nleft_1000 &lt;- sample_left(1000, sample)\nleft_10000 &lt;- sample_left(10000, sample)\n\nggplot(left_10, aes(x = perc)) + geom_histogram(bins = 5,\n  fill = \"gray\", colour = \"black\") + geom_vline(xintercept = 0.1) + \n  labs(x = \"Prevalence of left-handedness\", y = \"count\") +\n  theme_minimal() + theme(text=element_text(size=20)) \nggplot(left_100, aes(x = perc)) + geom_density(bins = 16,\n  fill = \"gray\", colour = \"black\") + geom_vline(xintercept = 0.1) + \n  labs(x = \"Prevalence of left-handedness\", y = \"count\") +\n  scale_x_continuous(limits = c(-0, 0.4)) +\n  theme_minimal() + theme(text=element_text(size=20)) \nggplot(left_1000, aes(x = perc)) + geom_density(bins = 30,\n  fill = \"gray\", colour = \"black\") + geom_vline(xintercept = 0.1) + \n  labs(x = \"Prevalence of left-handedness\", y = \"count\") +\n  scale_x_continuous(limits = c(-0, 0.4)) +\n  theme_minimal() + theme(text=element_text(size=20)) \nggplot(left_10000, aes(x = perc)) + geom_density(bins = 50,\n  fill = \"gray\", colour = \"black\") + geom_vline(xintercept = 0.1) + \n  labs(x = \"Prevalence of left-handedness\", y = \"count\") +\n  scale_x_continuous(limits = c(-0, 0.4)) +\n  theme_minimal() + theme(text=element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Close to the Unknowable\nAs you can see in the simulations above, a low sample size leads to an extremely broad spread in possible values for left-handedness (it’s between 0% and 40% with only 10 cases!), whereas an increased sample size produces values that are very close to the “true” value we chose.\nWhile there may still be cases where we mis-judge the proportion of left-handedness, a larger sample size means that these mistakes are closer to the “true” value, as extreme values get way less likely. As you can also see, the gains in accuracy provide diminishing returns at a certain point, with 1,000 responses already providing a pretty accurate picture.\nThis example not only works for variables with a known distribution. If we only draw enough random respondents from our pool, sheer randomness will almost* guarantee that we’ll end up with a distribution similar to the “true” distribution. Of course, the more variables (=dimensions) we want our data to be representative on, the bigger our sample needs to be - after all, we’re not only introducing a new variable as an additional range, but also every possible value combination between variables. This is another reason to keep the core research model (dependent and independent vars) as small as possible, in order to reduce the needed sample size.\n\n\n\n\n\n\nSide Note\n\n\n\n*Again: See the topic on significance for more on this.\n\n\n\n\nWhat Does our Sample Represent\nI’ve previously mentioned that the total population (Grundgesamtheit) represents the amount of people we theoretically want to reach and physically can reach. While the first part is usually pretty clear and thought about when doing research, the second part is sometimes forgotten, with dire implications for the underlying model.\nYou see, our random selection only works for people that can theoretically participate in our surveys. The personality traits of people who can’t even clear that hurdle cannot be randomized and are therefore probably not well-represented in the final data. This goes back to what I’ve said about data collection methods in the beginning: If you conduct an online survey, you will systematically miss certain parts of the population that do not have stable internet access or engage with online surveys (predominantly old people or socially disadvantaged). Therefore, our sample can only be representative for the people that would have participated if given the option.\nWhile this is probably obvious when conduction your own research, where this really matters is when working with other people’s data. You should always ask yourself “What was the data collection process and what groups were targeted?”, since it’s technically only these groups that your data can make claims about.\n\n\n\n\n\n\nImportant\n\n\n\nI will ramble more about this on the next page, because this issue is of central importance when it comes to significance.\nIt is an unfortunate truth that people use pre-selected samples to make generalized out-of-sample claims that they should not make based on the underlying methodology. While this usually works out fine anyway (surprisingly, when you simplify the world to general trends, many of the complexities of humanity fall away in the process), it is something you should keep in mind when you see someone making weird, over-generalized claims somewhere - they might be full of methodological shit.\n\n\n\nLast modified: 2023-09-20 11:14, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nKovel, Carolien G. F. de, Amaia Carrión-Castillo, and Clyde Francks. 2019. “A Large-Scale Population Study of Early Life Factors Influencing Left-Handedness.” Scientific Reports 9. https://doi.org/10.1038/s41598-018-37423-8."
  },
  {
    "objectID": "files/1stats_4significance.html#samples-and-analyses",
    "href": "files/1stats_4significance.html#samples-and-analyses",
    "title": "4  Effect Significance",
    "section": "Samples and Analyses",
    "text": "Samples and Analyses\nOkay, let’s start slow.\nWe’ve now seen that random sampling - if applied correctly - can be a way to get close to the “true” and usually unknowable values of some parameters or variables in the overall populations. However, while this increases the probability of landing close to the actual value, there is no guarantee of this. Going back to the left-handed example, we will very rarely land on the “true” value of 10% (in our example) - instead, if we conduct multiple studies with random sampling, we might find left-handed rates between let’s say 8% and 12%, with some samples reporting even more extreme deviations from the “truth”.\nThis is a problem, because the samples are technically correct, we were just unlucky in drawing them. This demonstrates that just because we do everything right in drawing our sample, we still might end up with an incorrect result!\nWe therefore need to extend our results reporting by another measure, which should ideally report something like a degree of confidence in the assertion that our obtained results are close to the underlying population’s “true” values. And this is exactly what significance reporting is all about.\n\nFlipping the Script\nThere’s a couple of issues with this simple idea though. To assess how large the difference between our sample and the underlying population is, we need to know the “true” value’s, well, value. And furthermore, to assess how likely or unlikely it is to obtain our sample’s value (i.e. the width of the distribution in our simulation), we would need to know the values of hundreds - if not thousands - of other samples to get to a robust result, because of the spread of values we talked about on the last page.\nLuckily, we don’t have to do that. If you recall what I said about the size of the distribution depending on the number of cases we examine, you won’t be surprised to hear that we can approximate the distribution’s width given our sample’s size. Clever mathematicians have figured out ways to estimate this degree of deviation - usually called the standard error - using only information about the sample, without any knowledge about “true” values required. We will get back to this concept at a later point, once we’ve established some of the mathematical and conceptual groundwork for actually calculating it.\nWith this knowledge about our sample’s mean and the sample’s mean’s standard error, we can now go back and - in a way - flip the script on how we approach things. Instead of starting from the “true” value and the way samples spread around it, we can calculate a probability distribution showing where other samples from the same population should show up if our sample is an average sample of the population:\n\nlibrary(ggplot2)\n# load sampling function from previously\nsample_left &lt;- function(rep_n, sample_n){\n  df &lt;- data.frame(sample = seq(1, sample_n), perc = NA)\n  for(i in 1:sample_n){\n    left &lt;- sample(c(1, 0), rep_n, replace = T, prob = c(0.1, 0.9))\n    df$perc[i] &lt;- mean(left)\n  }\n  return(df)\n}\n\n# sample distribution around known \"true\" value\nleft_1000 &lt;- sample_left(1000, sample_n = 250)\n\n# singular sample for example\nset.seed(19538) # keep randomness in check\nsample_1000 &lt;- data.frame(n = seq(1, 1000), \n                          left = sample(c(1, 0), 1000, T, c(0.1, 0.9)))\n# calculate sample mean and std.error\nsample &lt;- data.frame(mean = mean(sample_1000$left),\n                     se = sd(sample_1000$left)/sqrt(1000))\n\n# plot \"true\" distribution\nggplot(left_1000, aes(x = perc)) + geom_density(bins = 30,\n                                            fill = \"gray\", colour = \"black\") + \n  geom_vline(xintercept = 0.1) + \n  geom_vline(xintercept = sample$mean, colour = \"red\") +\n  scale_x_continuous(limits = c(0.07, 0.16)) + \n  labs(x = \"Prevalence of left-handedness\", y = \"count\") + \n  theme_minimal() + theme(text=element_text(size=20)) \n# plot estimated distribution\nggplot(left_1000, aes(x = perc)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"gray\", colour = \"black\",\n            args = list(mean = sample$mean, sd = sample$se)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"dark gray\", colour = \"black\",\n            args = list(mean = sample$mean, sd = sample$se),\n            xlim = c(0.1, sample$mean + (sample$mean - 0.1))) +\n  geom_vline(xintercept = 0.1) + \n  geom_vline(xintercept = sample$mean, colour = \"red\") + \n  scale_x_continuous(limits = c(0.07, 0.16)) + \n  labs(x = \"Est. prevalence of left-handedness\", y = \"count\") + \n  theme_minimal() + theme(text=element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\n\nI have manually chosen a case where our source estimate (based on a sample of 1,000 “people”, red) deviates noticably from the “true” average (black). As you can see in this example, the mathematical estimation based on the sample lines up pretty well with the actual deviations of 1,000 samples drawn from the population when it comes to the distribution’s width. This means that if we were to actually capture the “true” value with our sample, the standard error would be a great way to estimate how much other (hypothetical) samples would deviate from our chosen sample.\n\n\n\n\n\n\nSide Note\n\n\n\nBecause it’s based on a mathematical formula, the sample’s distribution looks way smoother than the “true” distribution (remember that that is based on actual sampling of 1,000 samples with 1,000 cases each). If we were to increase the number of samples drawn from the “true” distribution, it would appear smoother than here - I’ve simply opted to keep it to “only” 1,000 samples here.\n\n\nThere is just one more flaw in our logic. I’ve painted in the “true” value of 10% into the estimated prevalence so we can judge how far off we are - but as we’ve established before, there’s no way of knowing what this true value is beforehand, meaning this is a process you could never do with real research data!\nHowever, we can use what we’ve now created and theorized to create an approximation of this measurement of closeness. It will just require one more methodological twist…\n\n\nTwist on a twist\nBefore we go on with this, let’s highlight where we are right now. You’ll see later why this is important.\n\nWe started with a theoretical population that presumably includes everyone or everything touched by our research hypothesis.\nWe chose a data collection method that presumably captures every part of the theoretical population relevant to our hypothesis.\nWe took a sample from the population using our research method, which presumably includes random distributions of all parameters and variables important to our hypothesis.\nWe then stated that this sample is presumably representative of the average sample drawn from our population.\n\nWhat this gives us is an estimated value for our chosen parameter as well as a probability distribution for deviating values. Based on these values, we can now go ahead and calculate the likelihood of specific values to appear given our sample.\nHere’s what I mean by this: As we’ve seen here and in our previous simulations, if the sample size is big enough, most samples seem to fall very close to the actual population’s value. In fact, many of the samples almost perfectly overlap with the actual population mean, while deviations from the population mean in all directions get less and less likely the more extreme they are. This bell-shaped curve we can observe here has it’s own name - the Normal Distribution - and we will encounter it pretty often in the following pages.\nIf we take our sample value (it’s 11.4% left-handedness by the way) as the center of a simulated distribution - meaning we treat it as if were the true population value, other samples would tend to fall relatively close to it, just like with the the true value in our previous simulations.\nThat means if someone else comes along with an estimated 12% of left-handedness, we can accept that as pretty likely, since it’s relatively close to our sample’s result. If however another study claims 50% left-handedness, that should raise some red flags - differences this large are extremely unlikely, and therefore someone has probably fucked up somewhere."
  },
  {
    "objectID": "files/1stats_4significance.html#quantifying-differences",
    "href": "files/1stats_4significance.html#quantifying-differences",
    "title": "4  Effect Significance",
    "section": "Quantifying Differences",
    "text": "Quantifying Differences\nI’ve said before that these estimates can form the base of a probability distribution - and now you can probably see why: While most samples tend to land close to the true value, there is still no guarantee that any random sample will actually hit the true value perfectly (i.e. we get a sample that exactly results in our “true” value of 10.0000…% left-handedness) - or even at all. We therefore operate on a continuum between a 0% chance of perfectly hitting the truth down to every decimal and a 100% chance of getting any result at all.\nThe only footholds we have is that it seems based on our simulations that a) most samples end up very close to the actual value and b) deviations in both directions are equally likely.\n\n\n\n\n\n\nSide Note\n\n\n\nYou might have noticed that I’ve started dropping the quotation marks around the “true” population value. This is because they refer to different concepts: the “true” population value (with quotations) is an unknowable value in a specific distribution - in our case left-handedness - while the true population value (without quotations) is describing a hypothetical concept that may be conceivable, but has no actual, measurable value.\n\n\nWith these footholds and our previous estimations, we can calculate the probability of any sample ending up in a specified range around our distribution’s center. Using our sampled 11.4% and the associated standard error based on the sample’s size, we could for example calculate the estimated probability of another sample returning a value between 11% and 12% left-handedness. In this case, 11% and 12% would be the cut-off values for our calculation.\nBecause we established that sample deviations seem to appear equally in both directions, we usually take the same cut-off value for both sides and set it relative to the calculated distribution’s mean. In our example, this would mean that rather than calculating the probability of landing between 11% and 12%, we would instead calculate the probabilities between 11% and 11.8% or 10.8% and 12% left-handedness respectively, to keep both sides even. This also makes our reporting job easier, as we only have to report the deviation, instead of the actual values (which would be 0.4% and 0.6% respectively here).\nOf course, these cut-off points can be completely arbitrary based on our needs. We could for example take the “true” value of 10% as a cut-off point, meaning that our probability interval would go from 10% on one side to 12.8% on the other. In our plot above, this would be the dark gray area, with the black line serving as cut-off at the “true” value of 10%.\nIf we now calculate what proportion of space under the distribution is smaller than the cut-off vs. the total (i.e. the dark gray shaded area vs. everything dark and light gray) we get a percentage value (ex. 70% of the area is dark gray). This percentage value tells us the proportion of (theoretical) samples from the total population that we expect to land inside the darker shaded area delineated by the cutoff-point, with the cutoff-point being any arbitrary value on the variable’s scale. We can also flip this around to claim that with 70% “inside” the area, this means that there is only a 30% chance of a sample being as far or farther from our mean than our chosen cut-off points!\nHere’s where we can start to quantify our results: although we may not know anything about the actual, “true” distribution of left-handedness, we could for example make educated guesses based on previous research. If we found that previous research stated a left-handedness value of 8%, we could treat this value as one such cutoff-point, and see where it falls on the proportion scale. If we found out that - based on our sample’s distribution - 99% of predicted samples would fall in the interval 8%-(11.4%)-14.8%, this would mean that about 1% of samples would lie as far or farther away from our results as the previous research’s 8%! Because this is a very unlikely result, we would have grounds to claim that something is wrong here.\n\n\n\n\n\n\nImportant\n\n\n\nKeep in mind that this doesn’t have to mean that something is amiss here. We’re dealing with probability and chances, so everything is always somewhat possible. Also, this doesn’t mean that all prior research is trash just because we’ve found a discrepancy - it could also mean that there’s something wrong with our approach, as opposed to everyone else’s prior work.\n\n\nIf you think back to the first chapters, this might seem familiar to you. In fact, what we’re doing here is applied falsification: As discussed previously, we can never claim to have found the “proven” or “true” value of left-handedness, but we can try our best to disprove the established proportion given by prior research (or any other value for the proportion). In this case, we can use the 1% probability to convincingly argue that the prior value of 8% is probably an underestimation - although even that can never be decisively proven!\n\nConfidence Intervals\nDepending on the field and research tradition you’re in, there are custom guidelines for how different something has to be in order for you to claim that a difference is significant. Usually, values should lie outside a zone than encapsulates at least 95% of the calculated sample distribution in order to claim significant difference:\n\n# actual value within probability range (difference not significant)\nggplot(left_1000, aes(x = perc)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = NA, colour = \"black\",\n            args = list(mean = 0, sd = 1)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"gray\",colour = \"black\",\n            args = list(mean = 0, sd = 1),\n            xlim = c(-1.96, 1.96)) +\n  geom_vline(xintercept = 0.9) +\n  scale_x_continuous(limits = c(-3, 3)) + \n  labs(x = \"Value distribution with 95% intervals\", y = \"count\") + \n  theme_minimal() + theme(text=element_text(size=24)) \n# actual value outside probability range (difference significant)\nggplot(left_1000, aes(x = perc)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = NA, colour = \"black\",\n            args = list(mean = 0, sd = 1)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"gray\",colour = \"black\",\n            args = list(mean = 0, sd = 1),\n            xlim = c(-1.96, 1.96)) +\n  geom_vline(xintercept = -1.8) + geom_vline(xintercept = 2.5) + \n  scale_x_continuous(limits = c(-3, 3)) + \n  labs(x = \"Value distribution with 95% intervals\", y = \"count\") + \n  theme_minimal() + theme(text=element_text(size=24)) \n# actual value outside probability range, one-sided (difference significant)\nggplot(left_1000, aes(x = perc)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = NA, colour = \"black\",\n            args = list(mean = 0, sd = 1)) +\n  geom_area(stat = \"function\", fun = dnorm, fill = \"gray\",colour = \"black\",\n            args = list(mean = 0, sd = 1),\n            xlim = c(-1.645, 10)) +\n  geom_vline(xintercept = -1.8) + geom_vline(xintercept = 2.5) +\n  scale_x_continuous(limits = c(-3, 3)) + theme(text=element_text(size=20)) +\n  labs(x = \"Value distribution with 95% intervals\", y = \"count\") + \n  theme_minimal() + theme(text=element_text(size=24)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Note\n\n\n\nDepending on the quality of your data and the severity of your conclusion, you can argue for different confidence levels, as they are - again - entirely arbitrary. Especially for statistical analyses with low case numbers (&lt; 500), you will find some studies going to 90% confidence (you can tweak this even further with one-sided tests, see below).\nIf you’re dealing with life-and-death situations like in medical trials or with something that has the potential to revolutionize science as we know it (nuclear physics, …), it might be better to opt for a stricter confidence level of 99% or even 99.9%, to decrease the risk of mis-interpretation.\n\n\nIn these examples, the gray area encompasses 95% of the probable sample distribution. In the first case, our cut-off point is not different enough to land outside of the confidence interval, meaning any difference between it and our sample values could still be coincidental, based on different samples from the same data and therefore isn’t significant.\nThe second case shows two cutoff-points, of which one lies outside the 95%-range while the other does not. In this case, we could claim reasonable confidence in a difference between our sample and the right-side cutoff-point. While there is still a chance that all three values (sample mean and both cutoff-points) come from the same underlying data, this difference is big enough to suspect that something is probably amiss.\nThe third example shows something else entirely: Here, we rephrased our question about significance somewhat: Instead of testing both sides of the distribution (i.e. asking “is something significantly smaller or larger than our result?”) we only focus on one side, and ignore the other side.\nThis leads to an improvement in confidence intervals: 95% confidence and an equal split leaves 2.5% of samples below and 2.5% of cases above our calculated value. If we only focus on one side (i.e. we ask “is something significantly smaller than our result?”), we can shift the entire 5% “rest” over to that side. This is what’s shown here. Because we only focus on if something is significantly smaller, the right-side line is now non-significant, while the left-side line lies outside the gray zone - even though the right-side line is farther from our sample’s mean! Based on the way they’re constructed, these types of tests are also called two-sided or one-sided significance tests, with two sides (or tails) if measuring in both directions, and one if contrasting to only one direction.\n\n\nSeeing stars\nInstead of comparing data against arbitrary cut-off points, statistical programs usually compare your results against a pre-defined cut-off point: 0.\nThis is because most statistical analyses revolve around the answer to the question “Does X have an effect on Y?”, and since we can’t prove this (recall falsification), we instead try to dis-prove that X has no effect on Y - making zero the cut-off point we ideally want to fall outside of our confidence interval. This also covers inverted effects, since any journey from positive to negative (or vice versa) needs to cross 0 at some point.\nAs you might imagine, 0 will basically never line up perfectly with any pre-set confidence interval like “95%”, instead potentially producing weird or very small values. As such, it has become custom to represent significance with stars in analysis results, where more stars correspond to a “harder” test.\nYou will usually see these stars explained below the table of results, in a form like this: *p&lt;.1. **p&lt;.05. ***p&lt;.01. In this example, one star (*) would mean that the remaining probability of randomly arriving at a value as far away as 0 based on the given result was 10% or smaller - a 90% confidence interval - with the same going for 95% (**) and 99% (***) confidence intervals respectively."
  },
  {
    "objectID": "files/1stats_4significance.html#the-limits-of-significance",
    "href": "files/1stats_4significance.html#the-limits-of-significance",
    "title": "4  Effect Significance",
    "section": "The Limits of Significance",
    "text": "The Limits of Significance\nBear with me, this is going to be annoying.\nYou see, because significance is such a cool concept and because stars are easy to communicate (notice how I don’t use “understand” here), this is what many people look at whenever they use statistical methods without further thinking about the result. This also makes them the de-facto standard of any scientific reporting. But as you might have noticed already, this concept of significance is built on a foundation that can only be described as rickety at best and imaginary at worst.\nThis refers back to all the assumptions we made during the process of getting to the concept of significance. You know, the ideas of us reaching our target population, getting results that are representative of the actual average, presuming that we can generalize a probability distribution from our sample and all that jazz.\nIt turns out, it’s basically never the case that all these assumptions are met. Something is always going on somewhere that leads to skewed data or mis-interpretation of results. Does that mean everything we do is worthless? No. But it does mean that even the confidence intervals themselves are just estimations and not hard cutoffs! You can never be certain that your calculated 95% are the actual 95%!\n\n\n\n\n\n\nSide Note\n\n\n\nYou can estimate how likely it is for you to detect an actually real effect based on the effect’s theorized strength, your sample size and a couple other parameters. We will talk more in-depth about this concept (called statistical power) at a later time, but I want to mention it here already.\nStill, just like everything else, this is a probability - and based on how much science is out there, even the most unlikely things will have happened somewhere at some point!\n\n\nBut wait, it gets even worse. This is where interpretation of the probability values matter (and also where we clear up my messy wordings in the last few paragraphs). You see, testing with a 95% confidence interval does not mean that we are 95% confident that the result we calculated is the correct result! Instead, all we state is that if one were to draw a similar sample and conduct the same analysis again, we would land inside our “gray area” 95% of the time!\nNow you might argue that your regression did not produce a meager 95% confidence result, but instead left you with a confidence value of 99.99999%! To that I say “Great, good for you! But what does this actually mean?” All you’ve established now is that your calculated value is really, really likely to be less far away from your result than 0 is. What does this tell us? What benefit do we get from knowing that?\n\n\n\n\n\n\nImportant\n\n\n\nNow that we have everything in place, we can concisely state what confidence intervals and significance entail:\nGiven a set probability, a confidence interval is the range of potential values we would expect hypothetical re-calculations of our analysis based on other samples from the underlying population to take. This probability distribution is based on the assumptions that\na) our sample is representative of the “true” population values and distributions and\nb) that all remaining dynamics in the sampling process are completely random and non-systematic.\nConfidence is usually measured against the value “0” and states the proportion of random samples that would be less far away from our calculated value than the distance between 0 and this value.\n\n\nHere’s an interesting implication if you think this to it’s logical conclusion: Presuming a best-case where all our procedural assumptions about sampling hold as well as a 95% confidence interval, this would mean that around 5% of all published research depicts effects that are not actually “true” effects. Instead they are simply unlikely enough to have happened to be outside a non-effect’s 95% confidence interval!\nAnd that’s the best case. Things can and will go wrong, if you notice them or not. There may be some unknown systematic effects that change the true population you can make claims about (people without internet, households where everyone works during your call times, …), or our sample could be filled with only one-sided outliers, ruining our results. Or, worst of all, we cherry-picked or data to achieve significant effects in the ways we wanted in order to achieve the stars required to get our work into scientific journals (this is also called p-hacking, and it’s an issue)!\n\nBut then what shall we do?\nIn short? I have no idea. Significance might not be the ideal measure of what’s going on inside our data, but it is readily-available in almost all statistical programs and functions, easy to interpret and - unfortunately - the currency that makes the scientific world spin.\nI’m also not the first one to notice and address this issue: Over the years, scientists have cautioned against trusting significance too much (see ex. Cohen’s [-@Cohen1994] paper with the great title “The Earth Is Round (p &lt; .05)”), and a semi-recent plea (Amrhein, Greenland, and McShane 2019) to be more careful about mis-interpretation of statistical significance gathered widespread support.\nThere are other approaches to measuring the meaning of an effect, but until they catch on, significance still reigns supreme. The only thing we can do is be cautious when we encounter effects that seem too good to be true, as there might be something fishy going on.\nAnd also, don’t blindly trust in stars (this goes for real life and statistics!) - Just because one gets a star and the other doesn’t, that does not mean that an effect with a 90.01% confidence level is so much better than an effect with a 89.99% confidence level. In the same vein, non-effects do not necessarily disprove previous studies that found effects! Use your head and think about your results!\n\nLast modified: 2023-09-20 11:15, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. “Scientists Rise up Against Statistical Significance.” Nature 567. https://doi.org/10.1038/d41586-019-00857-9."
  },
  {
    "objectID": "files/2desc_1basics.html",
    "href": "files/2desc_1basics.html",
    "title": "5  Describing Things",
    "section": "",
    "text": "Last modified: 2023-09-20 11:15, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/2desc_2summary.html",
    "href": "files/2desc_2summary.html",
    "title": "6  Summary Statistics",
    "section": "",
    "text": "Last modified: 2023-09-20 11:16, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/2desc_3plots.html",
    "href": "files/2desc_3plots.html",
    "title": "7  Frequency Plots and Distributions",
    "section": "",
    "text": "Last modified: 2023-09-20 11:16, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/R_basics.html#launching-r",
    "href": "files/R_basics.html#launching-r",
    "title": "Getting to know R",
    "section": "Launching R",
    "text": "Launching R\nSo you’ve successfully installed R (if not, try clicking here) and are now wondering about how to work with it? Fear not, for I am here to help you out!\n\n\n\nR over GUI (left) or over system console (here: Windows)\n\n\nLaunching R for the first time if you’re not already familiar with coding languages might be daunting. No matter if you launched the console (right) or GUI version (left), you will find yourself presented with an arguably garish and hard-to understand console. Unfortunately, you and this console will have to become best friends, as this is where you will do all the actual work - however, we’ll get to getting you a nicer workspace soon, don’t worry.\nTechnically, this console is all you need to work with R: As you can see above, I’ve loaded a preinstalled dataset and calculated a linear regression in just two lines of code. You can also use this console to do some simple math, for example calculating the cost of tipping 15% on a meal costing 8.99€ or the number of seconds in a day:\n\n# meal + 15% tip\n8.99 + 0.15*8.99\n\n[1] 10.3385\n\n# seconds in a day\n60 * 60 * 24\n\n[1] 86400\n\n\nNote that usually, including spaces in your code will not change anything about the way it is processed, but can and will improve its readability!\n\nRestrictions of using R only\nHowever, while R is a mighty tool, it suffers when used in the way presented above: Saving your work for future reproduction becomes cumbersome as you will have to keep track of each individual command, and especially when working with more complex analysis models, you will sooner or later lose control over what variables and data sets you have loaded and how they are named."
  },
  {
    "objectID": "files/R_basics.html#Rstudio",
    "href": "files/R_basics.html#Rstudio",
    "title": "Getting to know R",
    "section": "A better environment: RStudio",
    "text": "A better environment: RStudio\nThis is where RStudio comes in. RStudio is what’s called an IDE, an integrated development environment that aims to improve working with R.\n\n\n\nRstudio layout\n\n\nYou might recognize one of the four panes presented above (you can get the fourth one by opening an existing script or creating a new one via the circled icon) - it’s the R console we’ve encountered previously, only now it’s tucked in nicely between a variety of other useful panes. Let’s quickly look at all panes and their respective purposes:\n\nThe code editor is where you will probably spend most of your time. Here, you can arrange the code you’ve written and save it as a script for easy reuse. You can also run your code by clicking the “Run” button in the top right of the code editor, or via the keyboard shortcut Ctrl/Cmd + Enter. Running will execute whatever line the cursor is currently in, whatever parts of your script are currently highlighted.\n\nThe R console is what we’ve already seen when looking at standalone R. While you can write and execute your code in the code editor, the results and potential messages will all be displayed in the R console, so keep an eye on this pane when you run something to see if it actually worked. It also provides access to other consoles, in case you need them for anything.\n\nThe environment pane provides an overview of all variables, user functions and data sets that are currently loaded and available. It provides an overview over both their names and dimensions (i.e. how many rows/columns a data set has, etc.) and can be used to quickly inspect data sets by clicking on the respective entry. It includes other tabs as well, but most used will be the “environment” tab as well as the “git” tab when working with GitHub or other code repositories.\n\nLast but not least, the fourth pane contains a variety of information like a inbuilt file explorer, a plot tab to view and save created graphics and - arguably most important - a help tab containing useful information. In R, basically every function is documented in detail, and to access this documentation, you simply have to type ?FUNCTION into the console. Running ?mean, for example, brings up the documentation of the mean()-function, showing that this function not only computes the mean of a given distribution, but can also be extended with optional parameters to trim a certain percentage of “outliers” before calculation.\n\n\n\n\n\n\n\nSide Note\n\n\n\nSince some special functions aren’t really named - think of arithmetic operators like + or dplyr’s %&gt;% pipe operator, you can’t use ?+ to access their help functions. Instead, you have to wrap them in quotes, like so: ?\"+\".\n\n\nBy the way, you can freely move and arrange these panes to your liking, but keeping the default view is also completely fine."
  },
  {
    "objectID": "files/R_basics.html#navigating-r",
    "href": "files/R_basics.html#navigating-r",
    "title": "Getting to know R",
    "section": "Navigating R",
    "text": "Navigating R\n\nAssigning data: Variables\nWhen working with data, you’re usually dealing with complex tables or lists that you need more than once. Typing them out every time would be cumbersome, which is why you can save them as an object using the assignment operator &lt;- to improve ease of use. You can save most code results as an object, including but not limited to numbers, plots or summary statistics. In the following example, I’ve saved a character string, a vector of numbers as well as a data frame of car statistics as objects.\n\ncharacter &lt;- \"green\" # one character\nnumber &lt;- c(1, 4, 2, 5, 6, 7, 9, 4, 6, 2) # 10 numbers\ncardata &lt;- force(mtcars) # a data frame\n\nI also highly recommend giving your variables short, but descriptive names so that you can easily identify what a given variable contains at any given time.\n\n\n\n\n\n\nImportant\n\n\n\nR does not care what type of data you want to save in an object. It will happily overwrite any saved object if you tell it to, no matter what the original data was or the new data will be.\n\n\n\n\nComments\nWhile not directly relevant to your code, comments are an important part of keeping your sanity intact. Putting a # anywhere in your code will cause all further writing in the same line to be ignored by R when running functions. This means that you can put comments in their own line (see the first code block on this page) or in the same line as your code (see the second code block on this page). No matter where you want to keep them, comments are a great way to document what you’ve done (and why you’ve done it), which will help immensely in understanding your code at a later date, or someone else’s code if you’re ever collaborating with other people.\n\n\nVectors\nWhenever you’re working in R, you will probably be working with either vectors, matrices or data frames containing certain data types. While there are other data types, those three will probably be the bulk of what you encounter.\nA vector is simply a list of connected items (this gets confusing, as there is also a list data type). Both character and number created above are vectors, even though character only has one item, “green”. As you can see, creating a vector is easy, as you just have concatenate a number of values via the c()-function.\nVectors can be navigated by using square brackets: number[1] will give us the first observation of our number vector (that being “1”). You can also subset vectors by specifying certain ranges or positions, i.e. number[2:4] will give elements 2, 3 and 4 of our vector, while number[9, 2, 4] will return the 9th, 2nd and 4th elements, in that order.\nVectors can be of any length, but cannot feature multiple data types (more on that later). Combining differing data types to a vector will lead to R guessing what data type you want and changing everything accordingly, which is a common source of issues!\nSee below: as soon as a character item is added to our number-vector, math is no longer possible.\n\nnumber\n\n [1] 1 4 2 5 6 7 9 4 6 2\n\nnumber[2] + number[5]\n\n[1] 10\n\nnumber &lt;- c(number, \"not a number\")\nnumber\n\n [1] \"1\"            \"4\"            \"2\"            \"5\"            \"6\"           \n [6] \"7\"            \"9\"            \"4\"            \"6\"            \"2\"           \n[11] \"not a number\"\n\nnumber[2] + number[5]\n\nError in number[2] + number[5]: nicht-numerisches Argument für binären Operator\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are familiar with “real” coding languages like Python or C, you might notice something odd in this explainer - and yes, you’re right: The first element of a vector is accessed via [1] because R, unlike those languages, does not start counting at 0!\n\n\n\n\nMatrices and data frames\nA matrix is the two-dimensional brother of a vector, storing data on two dimensions (commonly referred to as “rows” and “columns”). Just like vectors, however, matrix items across both rows and columns need to be of the same data type. Items inside matrices are also accessed via square brackets, however this time it needs two digits: one indicating row, and one indicating column. matrix[2,1] will for example spit out the element in the second row of the first column. To access an entire row/column, just leave the other entry empty (but keep the comma!): matrix[2, ] for the entire second row.\nWorking with real-world data, however, you will encounter data in different formats. For example, when working with survey data, you will find numeric values (age, …), character vectors (name, …) and other data types in the same dataset. While Matrices can’t be used tor this sort of data, data frames can. Data frames are basically a group of vectors of the same length, with related entries on the same position (i.e. the 5th “age” entry will refer to the same person as the 5th “name” entry). For this reason, data frames are probably the most widely used data type. (If you’re wondering why matrices are still important if data frames are so much better, the answer is simple: matrix multiplication is a powerful mathematical tool in many calculations.)\nJust like matrices, you can navigate data frames via square brackets - if “age” is the first column, then data[5, 1] will return the age of the 5th respondent. Data frame columns can also be accessed directly by name using the $ operator: data$age would return a vector of the entire age column, the same result as using data[, 1]. Getting the 5th entry also works both ways, with data[5, 1] and data$age[5] producing the same results.\nThis will be a theme with R moving forward: There is a variety of ways to achieve your goals, which way you choose will (mostly) be down to personal preferences.\nIf you are wondering how you could possibly know the names of the variables in your data set, there is again a variety of ways to achieve your goals, with the simplest being the names(data) function that lists the names of all columns, in order.\n\n\nData types\nWe have encountered numbers and characters before. We’ve also seen that you can do maths with numbers, but not characters, and we have also seen that when combining both, characters take priority. Numbers and characters will probably be the bulk of what you encounter, but there are a multitude of data types built into R, and some packages expand these definitions even further. Another common data type besides numbers and characters is the logical data type, which can take the values TRUE and FALSE.\nThis data type is a bit weird, as it usually has no use on its own, but can be used to subset other rows, as it can be the results of simple filtering equations. Running data$age &lt; 30 for example returns a vector of the same length as data$age, with values TRUE if the age is under 30 and FALSE otherwise. This can be combined with the square brackets above to filter data: data[data$age &lt; 30, ] for example will return a modified data frame, including the rows of respondents under 30 and all columns (since the second parameter was left empty).\nWhile not technically data types, NA is still something we should talk about. NA stands for “non-applicable” and will be used whenever there is no value to be displayed, be it because there is no value in the underlying data for that case (like accessing a row/column that doesn’t exist) or because some sort of function produces no output (like running mean() on a character vector). NAs aren’t inherently a bad thing, as they will be discarded for most calculations or can be discarded manually via optional arguments (mean() has a na.rm argument to ignore NA in the data), however, you should still be aware whether there are any NAs in your data.\n\n\n\n\n\n\nSide Note\n\n\n\nA few points to expand further on combining and transforming data types:\n1. While R - just like many other programming languages - features different types of “numbers” (float, double, …), you usually do not have to worry about converting these. R is pretty good at guessing which display type is the most appropriate for any situation.\n2. In case R cannot figure out your data’s types, it should default to treating them as character strings. This may be annoying since you can’t do math with strings, but they can contain basically everything - so you at least shouldn’t lose any data.\n3. In case you ever find that R wrongly converted something (or want to force a conversion, even if it means losing data), R has a variety of functions starting with as. to accomplish this: For example, as.integer() forces everything to be treated as an integer - anything that can’t be converted gets converted to NA values instead.\n\n\n\n\nFunctions\nIn R, functions are your bread and butter. R comes packed with a wide variety of functions, useful for finding the minimum value of a vector or calculating complex regression models. It can be expanded even more via packages (more on those later). Functions are usually lowercase and end with brackets. The mean()function we encountered before is one such function. Given a vector, it calculates the mean of all the items in this vector:\n\nmean(c(1,2,3,4,5,6,7,8,9,10))\n\n[1] 5.5\n\n\nAs we’ve seen when running ?mean, each function comes with several parameters for us to tweak. Each parameter has a name attached to it, but if no names are specified, R treats them in the order they appear. To illustrate: If we write mean(c(1,2,3)) R will interpret the input vector as the first argument (i.e. the vector to compute the mean for). It is identical to manually specifying the argument every time (i.e. mean(x = c(1,2,3))). Same goes for multiple arguments: mean(c(1,2,3), 0.2) and mean(x = c(1,2,3), trim = 0.2) produce the same result. Keep in mind that if you don’t specify argument names, you’ll have to keep within the function’s preset order of arguments, while when specifying you can also switch the arguments around (i.e. mean(trim = 0.2, x = c(1,2,3)))."
  },
  {
    "objectID": "files/R_basics.html#expanding-r-libraries-and-packages",
    "href": "files/R_basics.html#expanding-r-libraries-and-packages",
    "title": "Getting to know R",
    "section": "Expanding R: Libraries and packages",
    "text": "Expanding R: Libraries and packages\nWhile R comes prepackaged with a variety of functions, you will find yourself limited in many circumstances. For example, reading an Excel-file (.xls or .xlsx) into R is not possible with R out-of-the-box.\nFortunately, due to its open-source nature, everyone can write their own functions and commands to expand on R’s capabilities and publish them for everyone else to use. These collections of functions are commonly called libraries and are distributed in a set format called packages. There are a whole lot of packages available, and you will not need almost all of them.\nAs an example, R cannot natively load data from Excel-files (.xlsx, …). However, a variety of packages include functions to enable this, like the read_excel() function from the readxl package.\nYou can install packages onto your local machine at any time (as long as you have an internet connection) via the install.packages() command. To actually use the installed packages inside R, you will have to load them into your project via the library() command.\n\n\n\n\n\n\nSide Note\n\n\n\nSince you are matching names on a distant server in order to find your content, install.packages() requires a string (install.packages(\"name\")) to match.\nIf you’re actually loading an installed library, however, the library() function searches for a package object, instead of matching a string, meaning you don’t need the quotation marks (library(name) without the “)!\n\n\nKeep in mind that while you only need to install packages once (unless you want to update, which does not happen automatically), you have to load and reload your libraries every time you open up R and before you can run functions sourced from these packages. As such, I recommend putting the required library() functions at the beginning of your script for easier access.\n\nLast modified: 2023-11-09 15:00, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/R_ggplot.html#why-ggplot2",
    "href": "files/R_ggplot.html#why-ggplot2",
    "title": "8  Beautiful data visualizations using ggplot2",
    "section": "Why ggplot2?",
    "text": "Why ggplot2?\nEven though base-R already comes with multiple functions and features to visually display information (like the plot() and hist() functions), the results are usually not very nice to look at - especially when you think about publishing them in a more professional setting. ggplot2 is one of multiple packages (see also plot_ly) that aims to make data visualizations easy and nice to look at:\n\ndata(mpg)\n\n# plotting with base-R command plot()\nplot(mpg$displ, mpg$hwy, \n     # additional labels\n     main = \"Engine size - Miles/Gallon plot for different cars\",\n     sub = \"plotted in base-R\", xlab = \"Engine size (litre)\", ylab = \"Miles/Gallon\")\n# plotting with ggplot2 command ggplot() and dplyr-pipes\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(colour = \"dark gray\", fill = \"light gray\") + geom_point(aes(colour = cyl)) + theme_minimal() +\n  # additional labels\n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       subtitle = \"plotted in ggplot2\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nTwo things should be directly noticeable in the plots above: For one, ggplot’s plot is way cleaner and nicer to look at than the base-R plot, with options for colours and additional variables (number of cylinders) as well as measures for tendencies in the data. The other thing to note: ggplot’s command is way more complex and involved to set up than the base-R command.\nThis complexita is by design, as ggplot2 builds upon an underlying and standardized Grammar of Graphics basiert (the GG in ggplot). Luckily, much of what seems like complexities in the code above can be reduced down to the modularity this approach brings with it - and once you understand the core concepts and modules (or the core vocabulary of the “grammar”), you should be able to understand and write your own visualizations in no time. :)"
  },
  {
    "objectID": "files/R_ggplot.html#modularity-be-my-canvas",
    "href": "files/R_ggplot.html#modularity-be-my-canvas",
    "title": "8  Beautiful data visualizations using ggplot2",
    "section": "Modularity: Be my canvas",
    "text": "Modularity: Be my canvas\nggplot2 follows a modular design. This means that the final plot will not be calculated in one step, but instead built up through multiple overlaying partial commands. The amount and design of these commands is entirely up to you, and both the commands you pick as well as their order will impact the final plot. This enables you to build insanely complex plots if you invest the time and energy to learn the underlying ideas and structures.\n\nSetting the stage: ggplot()\nCore of this modular approach is the ggplot() command, which serves two functions: On one hand it specifies that everything following is part of a plot and outlines the general canvas, and on the other it can be used to specify global commands and settings that should be true for the entire plot. If you for example want to use the same core data set for all elements in your plot, you could specify this data set for each element individually, or you could include it in the ggplot() function, where you only have to type it out once.\nTo visualize: Executing an empty ggplot() command produces an empty canvas, in effect only providing a background for your plot (left). If you already include a data set as well as variables (don’t worry, we’ll look at this in detail in a second), you also already get axes overlayed depending on the scaling of your variables:\n\n# empty ggplot function\nggplot()\n# ggplot function with global data and axis specification\nggplot(data = mpg, aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\n\n\nThis specification of global parameters is also great to keep an overview over the variables and options used if you want to use multiple visualizations in the same plot (ex. points and lines as in the example above). On a technical level it doesn’t matter which approach you take, as both lead to the same result:\n\n# plot with local vars for each element\nggplot() + geom_point(aes(x = mpg$displ, y = mpg$hwy)) + geom_smooth(aes(x = mpg$displ, y = mpg$hwy))\n# plot with global vars for the entire plot\nggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Notes\n\n\n\n\nAs you can see, if you include parameters in the ggplot() functions, you can use the tidyverse way of giving only column names, whereas the individual functions need a direct reference to the data set used.\nYou can also combine both approaches, for example if you want to use the same data set (specified in global function) but different colour palettes for different elements (specified in the local functions) or if you want on element to use an entirely different data source.\n\n\n\nAs you can see, we can now easily expand our core canvas by chaining further functions with a + at the end of the ggplot() function. The command chain ggplot() + geom_histogram() + labs() would create a canvas (ggplot()), draw a histogram of the specified data on top of it (geom_histogram()) and then overwrite the automatically generated names for parts of the plot with the names specified in labs(). Because each element is evaluated separate from the others, it is important to close all opened brackets before a +!\n\n\nUsed data and aesthetics: aes()\nAs can be seen above, used data and other aesthetic arguments have to be specified within a aes() command nested inside the command for an element (or the global command). Correctly nesting this function is an essential step and usually the primary source of issues if something doesn’t work with your code. The aes() command can take not only variables to display on the x and y axis, but also variables that should be used to define other attributes of the plot, like colour or fill and a whole host of further options.\nHOWEVER: Just because arguments like colour and fill can be specified inside aes(), that doesn’t mean they have to. You can also specify these aspects outside of aes() if you want to set them to a specific value rather than base them on a variable:\n\n# colour inside aes -&gt; colour based on variable\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_point(aes(colour = cyl)) + theme_minimal() + \n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")\n# colour outside aes -&gt; colour takes specified value\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_point(colour = \"blue\") + theme_minimal() + \n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo preempt upcoming confusions: You will get errors when you specify a variable colour outside aes(), but you won’t when you specify a definite colour inside aes(). Instead, code like geom_point(aes(colour = \"blue\")) will make R look for the variable blue in your data - and if that doesn’t exist, it will simply treat all cases as having the same value for this attribute and assign the first colour of the standard colour palette - which is red. So, if you ever wonder why “blue” appears as red in your plot, this might be why.\n\n\n\n\n\n\n\n\nSide Note\n\n\n\nKnowing when to use colour or fill is a whole headache in and of it self and depends on the chosen visualization method. Generally, fill is used for the inside fill of an element, while colour is used for borders and edges - with geom_point() being the most prominent exception and using only the colour attribute for points.\n\n\n\n\nVisualization elements: geom_...\nAs you have seen in the previous examples, geom_...functions build on the defined canvas and offer the meat of data visualization. If you enter “geom_” in RStudio, you will see that the autocomplete function offers you a wide array of different options for plotting your data, from lines up to entire maps.\n\n\n\n\n\n\nImportant\n\n\n\nWhen choosing a visualization element for your plots, always keep an eye on the arguments the associated function takes! Especially bar plots and histograms only take an x variable as input and derive y from frequency distributions of x. Manually specifying y in the geom-function (or the global ggplot-function) will lead to errors and annoyances.\n\n\nIn theory, the modular nature of ggplot2 allows you to stack as many elements on top of each other. However, keep the modular buildup of ggplot in mind: The most frequent element will overlay any element specified prior to it!\n\n# dots after line, dots overlay\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(colour = \"black\", fill = \"dark gray\") + geom_point(aes(colour = cyl)) + theme_minimal() + \n  labs(x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")\n# line after dots, line overlays\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_point(aes(colour = cyl)) + theme_minimal() + geom_smooth(colour = \"black\", fill = \"dark gray\") + \n  labs(x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther central building blocks\nAside from geom_... functions, ggplot packs a whole load of additional functions to improve and/or modify the way you display your data. I have already used the labs()function in multiple plots to specify my own names for for axes, legends or the entire plot, rather than the defaults, which are derived from variable names in your input data. Further options using additional functions are scale transformations (using for example scale_x_log10() or scale_y_data()), different colour palettes (ex. scale_colour_gradient() or scale_fill_manual()for custom palettes), as well as overarching design themes for the entire plot, like theme_minimal().\n\n\n\n\n\n\nSide Note\n\n\n\nDon’t know if you’ve noticed, but these functions are available in many different versions for many different use cases. Anything you find under scale_x_... will have an equivalent under scale_y_... depending on which variable you want to transform. Same goes for the other arguments, with scale_colour_... and scale_fill_... containing much of the same functions, only applied in different contexts."
  },
  {
    "objectID": "files/R_ggplot.html#everything-all-at-once---information-overload",
    "href": "files/R_ggplot.html#everything-all-at-once---information-overload",
    "title": "8  Beautiful data visualizations using ggplot2",
    "section": "Everything all at once - information overload",
    "text": "Everything all at once - information overload\nAs I’ve said before, you can tweak a wide array of elements to your liking - I’ve simply focused on colour and fill as I think these will be the two most frequently used elements. But you can go further: Tweak the shape of points, the thickness of lines, the opacity of bars or whatever else you want! Heck, put it all together on one plot and make everything entirely unreadable - but undeniably unique:\n\nmpg %&gt;% mutate(cyl = as.factor(cyl), year = as.factor(year)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) + # X and Y variable\n  geom_point(aes(alpha = cyl, # opacity based on # of cylinders\n                 shape = year, # year as form for points\n                 colour = fl, # fuel type as colour\n                 size = class # vehicle type as size\n                 )) + \n  geom_text(aes(label = manufacturer), size = 2, nudge_y = 1.5) + # manufacturer as name\n  theme_minimal() + theme(legend.position = \"none\") + \n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       subtitle = \"plotted in ggplot2\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\")"
  },
  {
    "objectID": "files/R_ggplot.html#presentation-and-colours",
    "href": "files/R_ggplot.html#presentation-and-colours",
    "title": "8  Beautiful data visualizations using ggplot2",
    "section": "Presentation and colours",
    "text": "Presentation and colours\nAs you have already seen multiple times by now, you can change basically the entirety of a plot’s presentation and resulting graphics. This can be of great help when it comes to generating plots that present large amounts of data in a readable format. I want to use this short section to highlight two different approaches to make this customization even more powerful:\nFor colours, I very much like using Brewer palettes (use scale_..._brewer(palette=\"Name\") in ggplot) for discrete values and viridis palettes (use scale_..._viridis_c(option=LetterOrName)) for continuous values (viridis can also handle binned or discrete scales with …_b and …_d respectively).\nAlthough I am partial to minimalism and therefore like theme_minimal() a lot, you can also expand the range of available plot themes with something like ggthemes if you like excel too much. You can also create and use your own theme if you like. For an easier time, have a look at the ggCorpIdent package - however, keep in mind that Tufte (2001) is right when he says that minimal and clear visualizations are often superior to everything else.\n\n\n\n\n\n\nSide Note\n\n\n\nTo get on my little soap box for a moment, I think that everyone who wants to create good visualizations should read Tufte (2001) at some point. Not because you should follow everything he says, but simply to raise awareness for some of the issues in data communication. Don’t worry, the book isn’t that long and it features a whole range of pretty images. :)"
  },
  {
    "objectID": "files/R_ggplot.html#special-use-cases-and-tricks",
    "href": "files/R_ggplot.html#special-use-cases-and-tricks",
    "title": "8  Beautiful data visualizations using ggplot2",
    "section": "Special use cases and tricks",
    "text": "Special use cases and tricks\n\nSub-plots based on variables\nUsing facet_grid() (or the less strict facet_wrap()), you can create subgraphs based on variables in your data set. That way, you could for example compare value distributions between different points in time. This is another way to display additional data without visual clutter as we’ve seen it in the above image. Based on your specifications, you can enter up to two variables to build subplots out of, using the formula facet_grid(verticalVariable ~ horizontalVariable) - however, you can leave one argument empty, if you just want one variable to control the split: facet_grid( ~ horizontalVariable):\n\n# facet_wrap based on year\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(colour = \"dark gray\", fill = \"light gray\") + geom_point(aes(colour = cyl)) + \n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       subtitle = \"plotted in ggplot2\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\") + \n  facet_wrap(.~year)\n\n\n\n# facet_grid based on # of cylinders and year - notice empty fields for 1999 and 5 cyl\nmpg %&gt;% mutate(cyl = as.factor(cyl)) %&gt;%\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(colour = \"dark gray\", fill = \"light gray\") + geom_point(aes(colour = cyl)) + \n  labs(title = \"Engine size - Miles/Gallon plot for different cars\",\n       subtitle = \"plotted in ggplot2\",\n       x = \"Engine size (litre)\", y = \"Miles/Gallon\", colour = \"# of cylinders\") + \n  facet_grid(cyl~year)\n\n\n\n\n\n\n\n\n\n\nSide Note\n\n\n\nIn a case like this one, it would probably better not to use global axes, in order to better see differences within groups at the cost of visual differences between groups. to achieve this, you can set a scales parameter inside facet_... to “free_x”, “free_y” or “free” (both).\n\n\n\n\nDifferent axes in one plot\nThis is a niche case you may or may not encounter eventually, but I’ve had to deal with it twice and have been annoyed every time - which means I write this mostly for myself, so I can easily find it again.\nAs mentioned in the beginning, axes in ggplot are calculated globally depending on your data - even if you plug in different data sources, the result is a single, global scale depending on the minimum of the smaller variable and the maximum of the larger variable. To display data on two different scales at the same time, a bit of annoying trickery is needed: You simply have to scale up the second variable so that it fits inside the scale of the first variable, add a second axis for this variable, and re-transform the labels on this axis back to the original size via a command like scale_y_continuous(sec.axis = sec_axis(~ . FORMEL)). In practice:\n\n# data frame with two y values and x value \"month\"\nclimate &lt;- data.frame(month = 1:12, \n                      temp = c(-4,-4,0,5,11,15,16,15,11,6,1,-3),\n                      precip = c(49,36,47,41,53,65,81,89,90,84,73,55))\n\nclimate %&gt;% mutate(precip = precip / 8) %&gt;% # recode precipitation to temp scale\n  ggplot(aes(x = month)) + \n  geom_col(aes(y = precip), fill = \"light blue\") + # add precipitation\n  geom_line(aes(y = temp), colour = \"red\", size = 1.5) + # add temperature\n  # now recode the secondary axis back to the original scale\n  scale_y_continuous(\"Temperature\", sec.axis = sec_axis(~ . * 8, name = \"Precipitation\")) + \n  labs(x = \"Month\", title = \"Climatogram for Oslo (1961-1990)\") + # add title\n  geom_hline(yintercept = 0) + theme_minimal() # toy around a bit more\n\n\n\n\n\n\nGoing even further beyond\nAs you can see, the possibilities ggplot offers are (nearly) endless! And if you add further R-packages to the mix, you can build plots that are even more insane. If you want to be inspired, have a look at this: https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html\nIf you want to learn more, I encourage you to just toy around a bit with ggplot. Once you understand the core format and the underlying grammar, you’ll be able to generate great plots in no time. And if you’re ever stuck, the internet has a wealth of knowledge around solving specific issues, whatever they may be. For further reading on ggplot, I’d also encourage you to have a look at the ggplot2 webseite and the R-Cookbook for graphs. Oh and read Tufte (2001)!\n\nLast modified: 2023-09-20 11:16, R version 4.3.1\nSource data for this page can be found here.\n\n\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative Information, 2nd Ed. Graphics Press."
  },
  {
    "objectID": "files/R_gitsetup.html",
    "href": "files/R_gitsetup.html",
    "title": "9  Working With RStudio and Git(Hub)",
    "section": "",
    "text": "Why Git?\nHave you ever accidentally deleted a file and then not found a way to restore it? Have you ever overwritten some important detail in a document and not been able to recover it? Do these situations still haunt you in your dreams? It doesn’t have to be like that! Git is a type of version control for your machine, allowing you to manually set Checkpoints for your progress that you can restore at any time for the entire folder or individual files within it.\nGoing even further, GitHub is a online database where you can upload these checkpoints. While GitHub is the most popular repository, there are many others that build on Git or similar structures to offer version control of your files. Now you may ask yourself: “Why would I want to upload my personal projects to the internet?” Well, here are some of the arguments:"
  },
  {
    "objectID": "files/R_gitsetup.html#version-1-git-over-a-desktop-client",
    "href": "files/R_gitsetup.html#version-1-git-over-a-desktop-client",
    "title": "9  Working With RStudio and Git(Hub)",
    "section": "Version 1: Git over a desktop client",
    "text": "Version 1: Git over a desktop client\nIn this version, all the interactions with GitHub will be handled automatically by a chosen software, and you won’t have to worry about anything. While this can be very convenient, keep in mind that in this way version control only works with this client! Meaning that if you want to sync something not intended by the developers, you might be in for an annoying time.\nIf you want to go this way, there is a whole boatload of potential desktop clients to choose from. If you for example already use VS Code for other projects, there’s a Git integration built in and you’ll find plenty of resources online to set that up, as well as plenty of plugins to expand it. Going beyond VS Code, GitHub has it’s own desktop client that’s able to sync any folder on your device. However, the last time I used it, I found it to be very annoying and clunky. The better alternative in my view and the one I use pretty regularly is GitKraken. The Team behind GitKraken is also responsible for the VS Code GitLens extension if you want to upgrade its capabilities."
  },
  {
    "objectID": "files/R_gitsetup.html#version-2-git-on-your-system-and-in-rstudio",
    "href": "files/R_gitsetup.html#version-2-git-on-your-system-and-in-rstudio",
    "title": "9  Working With RStudio and Git(Hub)",
    "section": "Version 2: Git on your system and in RStudio",
    "text": "Version 2: Git on your system and in RStudio\n\nStep 1: Installing Git\nAs mentioned previously, GitHub is only a web service for hosting you checkpoints and data. To communicate with GitHub, you can use a range of standardized commands and operations made available via a software called Git. Unfortunately (and this is the reason why desktop clients exist), Git is code-only and requires a Unix-terminal to function properly - which means Windows users have to jump through a few hoops when compared with everyone else.\n\nGit on Linux/Unix\nBased on your chosen distro, Git can easily be installed via package managers or built from source. For an installation overview for many of the more common distros see: https://git-scm.com/download/linux\n\n\nGit on MacOS\nIf you’re lucky, Git might already be installed on your machine from the get-go. To check if this is the case, simply open the terminal app and enter git --version. If a version number is displayed, git is installed on your machine - and if not, MacOS should ask you if it should install it now.\n\n\nGit on Windows\nWindows - unlike the other two options - does not have built-in Unix support, which means we also have to install a Unix terminal. Fortunately, Git comes with a built-in terminal (Git Bash) if you install it from the official website\n\n\n\n\n\n\nImportant\n\n\n\nGit will ask you during installation for a standard editor. In principle this choice shouldn’t matter, as that editor will only be used in case of errors, which you shouldn’t encounter when using Git through RStudio or desktop clients. Still, if you don’t know what you’re doing and have never heard of VI or VIM, it’s probably best to switch this to something like Nano or Notepad++ (if installed). All other installation steps can be left as default for normal use.\n\n\n\n\n\nStep 2: Git and RStudio\nAfter you install Git, RStudio should automatically detect it. To check, you can go to Tools -&gt; Global Options -&gt; GIT/SVN and see if it is listed there. If it isn’t listed there, you can manually add it by specifying the path: yourGitFolder/bin/git.exe. Restarting RStudio might also lead to a Git detection, especially if RStudio was opened during the Git installation process.\n\n\nStep 3: Configuring Git and GitHub\nAll the following steps can be done via the previously installed Git Bash terminal - however, this is (in my view) an unintuitive and error-prone process for casual users. Luckily, there is a way to configure Git in a more user-friendly way using R’s usethis package. After installing and loading usethis, you can open Git’s configuration directly using the edit_git_config command. Enter your name and the mail address of your GitHub account as follows:\n[user]\n    name = Your Name\n    email = github@mail-adress.com\n\n\n\n\n\n\nSide Note\n\n\n\nThe name you set here can be anything you like - however, keep in mind that it will appear whenever you upload something to GitHub from this device. You could for example give a name like Paul work laptop, to easily track which device made the changes.\n\n\nAfter this step, Git is ready to serve as a local version control. However, to sync your changes with GitHub, one more step of authentication is needed - after all, if it were only about name and mail address, anyone could commit changes in your name.\nLuckily, the usethis package can help us here once again: Running the function create_github_token() should open up the correct GitHub website to create your own so-called Personal Access Token. Here, you can also specify whether this token should only be active for a limited time (useful when working on other people’s machines) or what privileges this machine should have (touch this if you know what you’re doing, otherwise defaults should be fine). After completing this step, GitHub should present you with a token of the format ghp_....\nNote down this token somewhere, as you will never see it again once you close the site!\nTo set this token on your machine, go back to R and run the function gitcreds::gitcreds_set() (installed alongside usethis) and enter the token you just created. Theoretically, everything should be set up - you can check that yourself by running git_sitrep(). The big advantage of this is that Git is now globally configured and associated with your GitHub account, no matter what program you want to use on your machine - if it has Git or GitHub functionality, it should automatically detect your configurations.\nCongratulations! You should now (hopefully) have everything set up and ready to use version controlling and GitHub! In the next step, we’ll be looking at actually working with Git inside RStudio, as there is a lot of neat functionality built-in to take work off of you.\nIf you want to know more or take a deep dive on using R and Git - or if you encounter any problems that require a more thorough explanations - I recommend you have a look at https://happygitwithr.com/\n\nLast modified: 2023-09-20 11:16, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/R_gituse.html",
    "href": "files/R_gituse.html",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "",
    "text": "Starting a project\nTo have Git integration work from inside RStudio, it is strictly necessary to work inside an RStudio project rather than just a loose collection of code files scattered around your machine. This is because RStudio seems to treat the .Rproj file as an anchor point to build the file management around. While there are many ways to start a project, I want to highlight two different ways of doing things - although both boil down to the same method in the end.\nNow that you’ve opened your Git-synced project in RStudio, you might have already noticed something new: The Environment tab (usually top right) features a new tab simply labeled Git. When working with Git in RStudio, this tab is where all the magic happens - and everything that you otherwise would have to execute manually via the aforementioned Unix-terminal. Don’t be scared, but I will now show you a graphic visualizing the basic working steps in Git that I stole from Reddit:\nYou should already know many of the areas mentioned in this image, although maybe not by this name: The remote branch is the repo saved on GitHub, while the working branch is the file structure on your current machine. The other layers in between are what makes Git so powerful for version control (and so irritating in the beginning). Here’s how it works on a basic level:\nNow, if you’ve already made some changes to your new R project, you might notice that things start to appear inside the git-tab un RStudio. Based on the type of change, you should see a coloured symbol appear in the “Status” column: ?(New file), A(dded), M(odified), D(eleted) and R(enamed) will probably be the ones you’ll encounter most frequently. This window is RStudio’s pendant to the staging area I just mentioned - here, files are being prepared for a git synchronization. What the command git add does in the image above can be achieved here by clicking the check box:\nTo commit your selected files, you simply have to click the aptly named “Commit” button, leave an informative commit message as to what you did and why (so you know what happened when you look back in a few weeks/months/years) in the now open window, and then press “Commit” there as well.\nCongratulations! You now know how to use Git for version control! All that remains now is syncing your files with GitHub - something RStudio directly lets you know by complaining that your local branch is ahead of GitHub (here called origin/main) in it’s update history. To rectify this, simply press the green, upwards pointing arrow. In the opposite situation, where GitHub is further ahead than your local machine, RStudio would still complain, and you would pull by pressing the blue, downwards pointing arrow."
  },
  {
    "objectID": "files/R_gituse.html#new-project-that-started-on-github",
    "href": "files/R_gituse.html#new-project-that-started-on-github",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "New project that started on GitHub",
    "text": "New project that started on GitHub\nThis in my view is the cleanest way of organizing version control, as much of the setup headaches get handled automatically for you:\n\nGo on GitHub and press the nice + button at the top to create a new repository\nOpen your newly created repository and press the green Code button at the top. Copy the web address displayed here\nOpen RStudio, start a new project (File -&gt; New Project), select Version Control -&gt; Git and enter the link you copied into the Repository URL field.\n\n\n\n\n\n\n\nSide Note\n\n\n\nTo keep things organized, I highly recommend selecting a dedicated folder for coding projects on your machine: If you input this folder at Create project as subdirectory of, RStudio will remember this and automatically select it the next time you generate a new project.\n\n\n\n\n\n\n\nThe big advantage of doing things this way is that the selected GitHub repository will be seen as the origin without needing manual setup. In this way, the project should immediately be usable, without having to worry about manually pushing or pulling files. If there are already files in your chosen repository, these will also automatically be downloaded and made available locally."
  },
  {
    "objectID": "files/R_gituse.html#preexisting-local-project",
    "href": "files/R_gituse.html#preexisting-local-project",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "Preexisting local project",
    "text": "Preexisting local project\nDo you already have some old project lying around that you want to update and share and/or save on GitHub? Then this is the step for you! The core idea in this step is the same as before: We follow the instructions above to generate an (empty) repository and generate it locally as a new project. The trick with this approach is that you can now go ahead and - after opening the project’s folder in a file manager of your choice - simply copy all your old files into this new folder. RStudio’s Git integration will recognize these files as new to the project, and you will be able to work with them as if you had just created them inside the project."
  },
  {
    "objectID": "files/R_gituse.html#branches-more-control-over-your-project",
    "href": "files/R_gituse.html#branches-more-control-over-your-project",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "Branches: More control over your project",
    "text": "Branches: More control over your project\nWhen you start a new project, you end up with one linearly developing history of changes, usually called a main branch (or “master” if following older conventions). However, it might not always be a good idea to work inside this main branch. For example if you’re only testing features and you’re not yet sure if they make it into the final project (or if they are horribly unstable), it might not be a good idea to include them in your main code database where they might ruin stuff or at the very least fill up your change history. Another case would be if you wanted to try and compare multiple different approaches to the same problem - you can’t really have both in the same file at the same time usually, as they are exclusive.\nLuckily, Git’s designers have thought of this and introduced the concept of “branches”: versions of your code that “split off” from the main branch and can develop in differing directions, allowing you to try out all the wildest ideas without ruining your project. For a practical use case, as I’ve mentioned before, ths website updates automatically whenever I commit something to the underlying repository. Now, I don’t want this to happen any time I make and commit some minor change, which is why I actually commit to a different branch called “indev” - and once I’m satisfied with my progress, I simply carry over all my changes to the main branch in one fell swoop.\n\n\n\n\n\n\nSide Note\n\n\n\nEven if you’re not dealing with these kinds of issues, it is probably always best to work with a separate testing branch, and NEVER commit untested code directly to the main branch - who knows what you might break in the process …\nTo make this easier to remember, if you open GitHub, you should see a message complaining that your main branch is unprotected. Setting the option to at least require pull requests (see below) is probably a good idea.\n\n\nWhen working in R, I found it easiest to employ the following workflow for new branches:\n\ncommit all outstanding changes to the branch you want to branch off from\ncreate a new branch on GitHub\nPull in GitHub. Since you’re up-to-date, no files should be overwritten, but the new branch should be detected. You can now switch local branches inside the Git tab (where it says “main” in the image above), and all changes should automatically be attributed to this branch from now on."
  },
  {
    "objectID": "files/R_gituse.html#forks-whats-yours-is-mine",
    "href": "files/R_gituse.html#forks-whats-yours-is-mine",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "Forks: What’s yours is mine",
    "text": "Forks: What’s yours is mine\nIf you’ve ever taken a stroll through the wonderful world of open-source software or had the time to look at GitHub’s “Explore” section, you might have noticed something: There is a whole boat load of complete, incomplete and abandoned software available out there. In fact, if you’re looking to solve a specific problem, you might find that someone has already started working on it - or completed a somewhat similar project that just needs some slight tweaking for your use case. This is where forks come in.\nForks are basically just like branches, with the main difference being that you’re not creating a new branch inside the same repository, but instead creating a new repository based on someone else’s work. That way, you start out with all the work they’ve already committed and can then go on to tweak it to your liking. For a practical example, the engine underlying the Firefox web browser has almost 2.000 forks from users modifying security settings to their liking or building their own web browsers from the provided baseline (not an endorsement, just an example)."
  },
  {
    "objectID": "files/R_gituse.html#merging-why-cant-we-be-friends",
    "href": "files/R_gituse.html#merging-why-cant-we-be-friends",
    "title": "10  Using Git(Hub) in RStudio",
    "section": "Merging: Why can’t we be friends?",
    "text": "Merging: Why can’t we be friends?\nNow that you’ve substantially improved someone else’s code (or toyed around in a separate branch and want to update the main branch), another step is necessary: merging the changes. If you’re the owner of both the original (main in my case) and the new and improved branch (indev in my case), this is not an issue and can be done with relative ease. Out of convenience, I will demonstrate this using GitHub, but as before, you can also use Git via terminal.\nTo merge two branches via GitHub, go to your project’s repository and in the “Pull requests” tab click on New pull request. Now select the new branch, and you’ll land on a comparison website. It should have already selected a comparison between indev and main at the top (or however you called your branches) and show you the changes between both branches. If you toy around with this, you can see that you can merge any branches in your repository - you aren’t limited to constantly overwriting your main branch.\n\n\n\n\n\n\nSide Note\n\n\n\nThis allows for more complex testing structures. You could for example run a “main” branch with working code, an “indev” branch to develop new features that branches off from “main” and an “unstable” branch off of “indev” that tests brand new features. Once you’re reasonably certain about your code, you could then merge “unstable”’s changes into “indev” for further testing, before deploying them to “main”.\n\n\nIf nothing went horribly wrong in your branch management, you should also be presented with a big green “New pull request” GitHub should automatically open a overview site for the new pull request, where you simply have to approve the changes to incorporate them to the main branch (or whichever branch you want to merge to). This is also where requests from other users that worked on your code will appear for your approval (and where your pull requests will appear on other users’ projects.\nAs before, if you’re still unsure about how things work or want to learn more about the possibilities Git and R offer, I recommend you look at: https://happygitwithr.com/index.html\n\nLast modified: 2023-09-20 11:16, R version 4.3.1\nSource data for this page can be found here."
  },
  {
    "objectID": "files/references.html",
    "href": "files/references.html",
    "title": "References",
    "section": "",
    "text": "Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019.\n“Scientists Rise up Against Statistical Significance.”\nNature 567. https://doi.org/10.1038/d41586-019-00857-9.\n\n\nField, Andy P., Jeremy Miles, and Zoë Field. 2012. Discovering\nStatistics Using R. London ; Thousand Oaks, Calif:\nSage.\n\n\nKovel, Carolien G. F. de, Amaia Carrión-Castillo, and Clyde Francks.\n2019. “A Large-Scale Population Study of Early Life Factors\nInfluencing Left-Handedness.” Scientific Reports 9. https://doi.org/10.1038/s41598-018-37423-8.\n\n\nPopper, Karl. 2002. The Logic of Scientific Discovery (Routledge\nClassics). Routledge.\n\n\nRobitzsch, Alexander. 2020. “Why Ordinal Variables Can (Almost)\nAlways Be Treated as Continuous Variables: Clarifying Assumptions of\nRobust Continuous and Ordinal Factor Analysis Estimation\nMethods.” Frontiers in Education 5. https://doi.org/10.3389/feduc.2020.589965.\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative\nInformation, 2nd Ed. Graphics Press.\n\n\n\nLast modified: 2023-09-20 11:25, R version 4.3.1\nSource data for this page can be found here."
  }
]